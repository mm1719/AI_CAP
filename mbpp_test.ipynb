{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PajsMulQG8xD"},"outputs":[],"source":["%%capture\n","# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install xformers trl peft accelerate bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6236,"status":"ok","timestamp":1716597630515,"user":{"displayName":"CAP AI","userId":"15143934473145264642"},"user_tz":-480},"id":"WcTGFyNjOZDi","outputId":"33a3fa56-4245-4ff0-fcec-d0ce81d9be23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/380.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/380.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna)\n","  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","Installing collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453,"referenced_widgets":["ff73c7aaa91d42c59a6e8fc6dc20fd78","d520b15be0d24cc78c95014dcf6b0e4e","e4dc289256e2454ea124fd2f94a219a9","b80bc51b209a45ba97991563a63ed266","393948ac709e425e82aa4cb7cc20e2b1","d3d7925c4d834dcda23820758b76c4ed","7a9933e5437b4808b1fe8e588d78be73","63f92702c6944bccb5e280f2db14099e","9619737fc5c445e895208da566bb78c5","f82943104a2b4cd5965561bc8ba06295","3c27f78b904f4965940d69218fdc8a2a","ef2caf141f4349fb9f4b1dcc04dc3138","5332fa3aad6c45158c0345b9e71c8d7c","6541b8aa027345249b0fd59855748046","8d9daf91332f4137be3f940138cc671b","18c48600ee9c497597d0cb4d756c7d8e","bc3734ded12b48afb1de1622f93a0949","b2b32ebfb07b48478390a4ba837ed753","e3b7bf607a23468990aaccfe3b899e3b","095db7bb0bda4d9e8636650b71b17daf","f8ffdff543e146b0bd4a0cb10f042eff","7a8145f311ea41139216cd23fba73240","f510cd61b20b4595aa284f156943ee6f","d4cb8d2f8a5946cda51beb352a88991f","5f8f0a2374b643548cb65644aa577229","e24ac53c19bf4ae6b6df0e1f88b99b06","f2ab2556362447e4b072c2b4490d46d7","5740eeaf11cf4aa5989a4882b716c8ab","a4bd50bc9a624e71a766974770ccc5d3","dd6ace092a2544cd876cd9ec7f430c5a","63d9ddb6f8de48c694b1562ad5197ab0","b53a58944dbf45c7a9cf885bf9894524","2b4f5d808e494e48b4405db24e610020","51925c33bb384c4a9e1c3cec1c7f44af","f616448239c240eabc05b0b13c6d04ed","b7dd4a3df6d246a9813b017e94bc4cee","8ba1497dd5e440ed858569e115c1182c","315b21fecd9a45babb7aa54c9a2d9cb7","2b51255d569946c492f74feb5f078465","7ff29d6f292d4bbd9196519fd65c62ee","a0333f11b277466c93b8ae64d3ebe462","b52559c9a6144fccb78cf8c6ca6ff07f","279f6e84c91145be9bf6cd217eb4ed9d","c15a6ed7b1cb4f1c97c6100b2ffa19ae","329122862252485f91435e533bd1f3fa","36a71747beac4c37aaa43e49723920c4","3f9581d535c44d04979035e325bea57d","2b77364d071a4399a2aa0935153a5ebe","2ee8d52ca37b4674a6151a9086ca210f","771d167b06d14d4093e945fb41e2e0af","a25d07749b2b42bfb5c5a44a400c5be1","b4371118dcdd48b5b3e3d99cc65c8dcb","4b0e031c85a7496599f0b00c3c14db69","1d18e1487cf9436c8aba1d9ea11ece77","f623c036b7ad4f099deb4d61e31918d6","ff3bf4c47650467b901e6fa8f92c7783","030f5f4701704fa0847b11404163893f","075ab2040b0a4273bb26337e3ec5e690","6cf8cb82e35f4d05bd8eae1092291395","8e0ed61be4264d41947c33bfff79f423","2bca29ef67d644dca17ab66317670e1e","244dc21f57e44a089cb4ba734fc67a09","607db43475cb497fa5bba38de38ac654","1af4b37bceb14ac8950dd0d34e234c0a","551d0ebc36fc4b3b83b9ae4fd6b17316","f6e0287731ba45f6b939915120f60ad1","6c562645c20d4f49bac008da9f72aac1","0bfc47f69c4b482584c830fecef2d148","e30d2b9b7cd945acb7ef00973f910eb2","a13cff2d215c4747aa891552f6d2746d","b28875a4cc9246e99b3eadc0003104d9","3529180d681248dc89f3d1c61e5269d8","d396e4da70cb4b0c9e72a07c8430c17f","0db537b9d0e7401baf0f8335cdcb4603","bf75a0ecaa664b4fb39d7e872a64eabb","a1b46b0baed3405abb91efbf392041f2","75bd2d9b8b1348a1bdc673e9aacb3db8","0c25d82134ed446aba34f2b70f7ece02","a0683223056247d29851e7afb2a53d03","e1ca433b561d4981b72792c0394205ea","ac4a298d39cd471c9b084f5499166c44","715a22b4d22b4f1dbf2b33024f16e7c8","7e190b5de92c40baa7a625d028eb0748","729bbd6704d443b88aa56aa6733bc65a","e3d73b89a0694cb190550229b490510d","4e39ca33a3d64683bd4ec5355608f8b9","cef1f59c964b4309866a91ddb7a323a4","7889a111a3984d8b8181fe748dbdacb8"]},"executionInfo":{"elapsed":28676,"status":"ok","timestamp":1716597659185,"user":{"displayName":"CAP AI","userId":"15143934473145264642"},"user_tz":-480},"id":"qVg42Ed1Gwal","outputId":"e49045f4-7e25-4c8c-9ece-847d5861537c"},"outputs":[{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: You passed in `unsloth/Phi-3-mini-4k-instruct` and `load_in_4bit = True`.\n","We shall load `unsloth/Phi-3-mini-4k-instruct-bnb-4bit` for 4x faster loading.\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff73c7aaa91d42c59a6e8fc6dc20fd78"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2caf141f4349fb9f4b1dcc04dc3138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f510cd61b20b4595aa284f156943ee6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51925c33bb384c4a9e1c3cec1c7f44af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329122862252485f91435e533bd1f3fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff3bf4c47650467b901e6fa8f92c7783"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c562645c20d4f49bac008da9f72aac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c25d82134ed446aba34f2b70f7ece02"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n","fourbit_models = [\n","    \"unsloth/mistral-7b-bnb-4bit\",\n","    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n","    \"unsloth/llama-2-7b-bnb-4bit\",\n","    \"unsloth/gemma-7b-bnb-4bit\",\n","    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n","    \"unsloth/gemma-2b-bnb-4bit\",\n","    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n","    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\n","] # More models at https://huggingface.co/unsloth\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OchqLjkKGwap","executionInfo":{"status":"ok","timestamp":1716597672620,"user_tz":-480,"elapsed":13439,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"colab":{"base_uri":"https://localhost:8080/","height":305,"referenced_widgets":["1b823e9e45164ef58b07bde79760bca7","ebf3399593154b0faf39fb473a87e421","c00ca5f8c2464347abf91bbfb597de55","e884c9fb585045d4888f0447d7fedf7a","443e415ca87043ac9a7f46c7ba67063b","d1bdd55fe3e3425a972e3d8122ae988e","03f03c2030c44621b02df2f50f57e24b","78edb7885db94b0a95d00f3e8b5d5c01","0bb2cf3a8c5d4bb0923e38e95aa83277","18900b4b112045658e239a8e64f66673","e03c120d3f904204ad2c9117865d9de3","65e2a305aed547e48298c68e1f892eca","df54b038c0d94595bcf14c8cca75df76","1b057050db8c4f998e5f676c87f49484","9ee83c505672479eb703cb87219cad58","7d8c8e3c4a7d45228b154ee9b649374b","4313b0efb6eb4d78af92b3c7834751db","881da92665664b33962905ed2c3dca28","7cbde20ad68140f8809c7dc7e05a8c1c","491a295bc9624c2dab567d29913a658d","6acd59be62ed41d58a30b34f85be961c","a7c134bce0c946408a7330b791d10323","708c6617df3c4652bf74e542790a640e","5a9999f10c3241cdaa2e8c4686416d2c","5f842bb260794ba48dd6ec2d849fa26e","25077d3e29c542fe969cbf2c1257df07","2d2fa26f06764510b0776715d9bc9645","41841770156c48ddbf3dd7d220a16e88","150585bbfc804cbe8eba0a8c581e2ffa","d9ba483a7ffc4e0cabf1f4331c7761a0","a8db0d6b58354e6b8a5417ae98deed12","8eacf0103c044dbbb8df8b0951c0e30d","c8304cd0dd9448ae8d41e4065c7ac4c4","934bbfcfb7fb400d96f776c413556bbf","e315388f3fe344cea7f83471be15a0fe","6af68ec870ad407988a98b35cc009449","88b1655ff77f4c3dbebe2170331a8684","b84defb5396848cf80400c6551386506","1a409ef9816c4dea9b2a6ef85800f18f","d4f0411925b7436da65507b3a41de82a","c1c8c7abfa2541f4afe32e2d4de0a0f4","e4ab1c08d1474b7a9b76f04ec098acba","5b6b276b399b498e8d231ed7bb6312ac","5543b9d96f8343dcb7d2d607af2af124","9ab91447ac0f47749d7ed7da9135796d","86c4f0a181e44c1c860ec30345fac8d7","0f9e567e728347ffaaf3e237c715f177","8c93974a7c584c2ab69efeab5d404d93","a59c679f65364ba5b23d36ee2ea71359","64bb65a13d084f2c85e4cf23a648fa20","904359496d944ebc8240a884a40578b8","6a446bfc61e948d4bc5dd7c6ded60dbf","bf7dd10fdd944e92ad43b68101f48e66","feb26b7226484dd4b61372dae94d9f82","a135b0b5fe454338b5166087bb5377b6","5c3a8de2959643f792346b7c06989ad4","fea06c53d62446e5ab36c535fe74fe7b","da93b100650142bc9b3fca1e0b5be1b1","18a9d70146a54f479f1a2d96826ffb86","54ea885cb2a44adb8e98de3bd5bfa3f8","e5d0d485f92b486e9d482d6fd187f330","95c6e20578994042b0b6bf1abd6149da","d41655ca913b409fa2e176d61f4c9f0e","d4622415bc854189b6e9cd26670bebb1","b7e7aef4614942068556f842ad33e1e5","9e2f512ef3884d00938000f26eda9c65","81aa96a2fae94731bf06ca86fa2fdb9a","83713a08ae4648d2b231a7f53bdcca8d","551ec9da1998482eac1bbda910092b62","eba2ae5d86ee49f2bf2dab6c51daa065","61b26400e3534c7f8406a1a55c19f907","3f777c064b0d4deaa5baa1828be6b407","94eef1729690499ca57683820dbd5741","cfa45b10e61643a1ae0889b690a369b4","ce91d597e889485b9a05e9eb9cb0f855","c688781b68444b58844351aa3c0b6a41","79fa3c3b50d343b3b551602de1714f41","def3ff17361744d29d2f00afd72b1b90","359c531d5cff4aff8f3bfffd711ecee3","b4f30d2929544e14abaf8f0036ae00a2","be68e2eff47044909646d00699c8d26a","b6dc5a5c658b46dda5ba9bd8d55346f0","de726f613c814d7d94891709d5b879f9","6aaf39659c984e72abda911ec3d12414","b30fd9fcb6c14e479f37c5411ecb86a7","52ec704a9ecc40589c11cd9bb2ead791","19e1c896fa4242ccbdbe2f2a29af33d0","7770d502e5d54a95b910c4112a3ef751","c09258496b4d4a0198c86ef193587a8c","9a1cb54c6f5e4cfc948badf0cae27019","d32231e4e905456f9df9f8cf13a2fe3a","fca2c3dcbab24c4c89f2bb69e27b9709","223bd175447448de934a2ce39758efcb","4a0a94c7ad0844579e85d4f0fd05a22f","4b10fd994aca4b399b2f34ff6a7f4e1e","18a8f3d9130540579164aeafff8cf197","58b6d2423abb49ca9499e0601d5114c7","95e6da2dd4e8410cb1dee56b7f856164","27bff28371c04d71a909846dd5c1c047"]},"outputId":"372415b6-0fa4-4934-a9f6-9f1dc0ca836b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/9.06k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b823e9e45164ef58b07bde79760bca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/87.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65e2a305aed547e48298c68e1f892eca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/116k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708c6617df3c4652bf74e542790a640e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934bbfcfb7fb400d96f776c413556bbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/7.88k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ab91447ac0f47749d7ed7da9135796d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/374 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c3a8de2959643f792346b7c06989ad4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81aa96a2fae94731bf06ca86fa2fdb9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/90 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def3ff17361744d29d2f00afd72b1b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating prompt split:   0%|          | 0/10 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09258496b4d4a0198c86ef193587a8c"}},"metadata":{}}],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"mbpp\", split = \"train\")\n","validate_dataset = load_dataset(\"mbpp\", split = \"validation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60EMF8CGGwap","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716597676277,"user_tz":-480,"elapsed":3663,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"outputId":"4c991dd0-26d9-4873-fc36-ec5ba1c31ee8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]}],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKjpHDb2Gwaq","executionInfo":{"status":"ok","timestamp":1716597676277,"user_tz":-480,"elapsed":6,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["ed98288ea7274895a2a9106cbb6aa6be","beeea5ade3ee42368d72947db60740b1","4687518616a243f381cea886678fca31","8db4fa4d68a744b6a746d3f17b504c26","e2dc0e27154b47bbb563037c5e3d9738","744735a2852c4bbb8dcb225853d5df38","db22faef0bd6460b991cf4d6e287af8d","c9a7706eb62f4d569ef847b77ba9268b","9a9ba3b4cbd14d7a856317ad05fa42a5","103b5cba1fe74379807234d64ac7ade4","8e13c462d9844dfcb4707a47c2b7ef5e","48f49e88ceca4a8998fe3e8b6c8dc6b9","cba1915fe1d64af1aa02abc8033ba69c","9611fa78c64242108da5730b0415cacf","8c6a28bfda0d452585d01edbbe26a2f8","b50dd8fe90194116bc9a75b83466d1c6","68ff6ddb260749b6934db8a4e81bbb4b","fadf9bf81380467c940325ec8d65e89f","141461ec61af4f658ebb8f83b1432442","9814cc9eba08428c9358a6a760fe0143","17dfa81146074b58b86986d1d499aa5f","f350099fbfa64f44b94f924e0bc3965a"]},"outputId":"1ec45a99-d870-47d8-f379-c6149bedbc0b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/374 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed98288ea7274895a2a9106cbb6aa6be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/90 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f49e88ceca4a8998fe3e8b6c8dc6b9"}},"metadata":{}}],"source":["import pandas as pd\n","\n","# Define your custom prompt template\n","alpaca_prompt = \"\"\"Write a Python code to solve the given problem, and the code should be able to run the test case.\n","\n","### Instruction:\n","{}\n","\n","### Test Case:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","# Define the EOS_TOKEN (replace 'tokenizer' with your actual tokenizer instance)\n","EOS_TOKEN = '<eos>'  # Example token, replace with tokenizer.eos_token\n","\n","# Function to format the dataset examples\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"text\"]\n","    test_cases = examples[\"test_list\"] #give all test case\n","    outputs = examples[\"code\"]\n","    texts = []\n","\n","    for instruction , test_case, output in zip(instructions, test_cases, outputs):\n","\n","        # Format the text using the template and add EOS_TOKEN\n","        text = alpaca_prompt.format(instruction, test_case, output) + EOS_TOKEN\n","        texts.append(text)\n","\n","    return {\"text\": texts}\n","\n","# Apply the formatting function to the dataset\n","format_dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names)\n","format_validate_dataset = validate_dataset.map(formatting_prompts_func, batched=True, remove_columns=validate_dataset.column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456,"status":"ok","timestamp":1716597741160,"user":{"displayName":"CAP AI","userId":"15143934473145264642"},"user_tz":-480},"id":"qfP29u7jGwaq","outputId":"b0c8265f-2a00-4816-f212-5817d4667d26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Write a Python code to solve the given problem, and the code should be able to run the test case.\n","\n","### Instruction:\n","Write a function to find the longest chain which can be formed from the given set of pairs.\n","\n","### Test Case:\n","['assert max_chain_length([Pair(5, 24), Pair(15, 25),Pair(27, 40), Pair(50, 60)], 4) == 3', 'assert max_chain_length([Pair(1, 2), Pair(3, 4),Pair(5, 6), Pair(7, 8)], 4) == 4', 'assert max_chain_length([Pair(19, 10), Pair(11, 12),Pair(13, 14), Pair(15, 16), Pair(31, 54)], 5) == 5']\n","\n","### Response:\n","class Pair(object): \r\n","\tdef __init__(self, a, b): \r\n","\t\tself.a = a \r\n","\t\tself.b = b \r\n","def max_chain_length(arr, n): \r\n","\tmax = 0\r\n","\tmcl = [1 for i in range(n)] \r\n","\tfor i in range(1, n): \r\n","\t\tfor j in range(0, i): \r\n","\t\t\tif (arr[i].a > arr[j].b and\r\n","\t\t\t\tmcl[i] < mcl[j] + 1): \r\n","\t\t\t\tmcl[i] = mcl[j] + 1\r\n","\tfor i in range(n): \r\n","\t\tif (max < mcl[i]): \r\n","\t\t\tmax = mcl[i] \r\n","\treturn max<eos>\n"]}],"source":["print(format_dataset['text'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":559,"status":"ok","timestamp":1716547271540,"user":{"displayName":"CAP AI","userId":"15143934473145264642"},"user_tz":-480},"id":"tZZ-eBdfGwaq","outputId":"09a49887-b9aa-40bd-d9b7-f20baa439bc5"},"outputs":[{"name":"stderr","output_type":"stream","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = format_dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size=1, # Smaller batch size\n","        gradient_accumulation_steps=8, # Higher accumulation steps\n","        warmup_steps=50, # More warmup steps\n","        max_steps=200, # More training steps\n","        learning_rate=1e-4, # Adjust learning rate\n","        fp16=not torch.cuda.is_bf16_supported(),\n","        bf16=torch.cuda.is_bf16_supported(),\n","        logging_steps=10, # More frequent logging\n","        optim=\"adamw_8bit\",\n","        weight_decay=0.01,\n","        lr_scheduler_type=\"linear\",\n","        seed=3407,\n","        output_dir=\"outputs\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mV807PjkGwar","outputId":"91e5755e-0b9f-4785-ab69-a65b8d2e1de1"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/60 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 1/60 [00:00<00:34,  1.69it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6765, 'grad_norm': 0.4043063819408417, 'learning_rate': 4e-05, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 2/60 [00:01<00:33,  1.73it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6765, 'grad_norm': 0.4041517972946167, 'learning_rate': 8e-05, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 3/60 [00:01<00:33,  1.71it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6734, 'grad_norm': 0.39830029010772705, 'learning_rate': 0.00012, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 4/60 [00:02<00:31,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6532, 'grad_norm': 0.3791935443878174, 'learning_rate': 0.00016, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 5/60 [00:02<00:29,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6694, 'grad_norm': 0.36962753534317017, 'learning_rate': 0.0002, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 6/60 [00:03<00:29,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.5359, 'grad_norm': 0.2202277034521103, 'learning_rate': 0.00019636363636363636, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 7/60 [00:03<00:27,  1.90it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.5546, 'grad_norm': 0.2458399087190628, 'learning_rate': 0.00019272727272727274, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 8/60 [00:04<00:27,  1.88it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.458, 'grad_norm': 0.1889711320400238, 'learning_rate': 0.0001890909090909091, 'epoch': 8.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 9/60 [00:04<00:27,  1.87it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.414, 'grad_norm': 0.18755610287189484, 'learning_rate': 0.00018545454545454545, 'epoch': 9.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 10/60 [00:05<00:27,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3464, 'grad_norm': 0.18330705165863037, 'learning_rate': 0.00018181818181818183, 'epoch': 10.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 11/60 [00:06<00:27,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3045, 'grad_norm': 0.174131840467453, 'learning_rate': 0.0001781818181818182, 'epoch': 11.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 12/60 [00:06<00:28,  1.70it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.2872, 'grad_norm': 0.18103374540805817, 'learning_rate': 0.00017454545454545454, 'epoch': 12.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|██▏       | 13/60 [00:07<00:27,  1.70it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.2257, 'grad_norm': 0.18240168690681458, 'learning_rate': 0.0001709090909090909, 'epoch': 13.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 14/60 [00:07<00:26,  1.74it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.2235, 'grad_norm': 0.21762631833553314, 'learning_rate': 0.00016727272727272728, 'epoch': 14.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 15/60 [00:08<00:25,  1.75it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.1761, 'grad_norm': 0.30429983139038086, 'learning_rate': 0.00016363636363636366, 'epoch': 15.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 16/60 [00:08<00:24,  1.77it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.1448, 'grad_norm': 0.28108277916908264, 'learning_rate': 0.00016, 'epoch': 16.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 17/60 [00:09<00:24,  1.76it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0912, 'grad_norm': 0.19444850087165833, 'learning_rate': 0.00015636363636363637, 'epoch': 17.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 18/60 [00:10<00:23,  1.77it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0702, 'grad_norm': 0.20384356379508972, 'learning_rate': 0.00015272727272727275, 'epoch': 18.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 19/60 [00:10<00:22,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0572, 'grad_norm': 0.257669597864151, 'learning_rate': 0.0001490909090909091, 'epoch': 19.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 20/60 [00:11<00:21,  1.87it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0414, 'grad_norm': 0.2127029448747635, 'learning_rate': 0.00014545454545454546, 'epoch': 20.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|███▌      | 21/60 [00:11<00:21,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0319, 'grad_norm': 0.2578922510147095, 'learning_rate': 0.00014181818181818184, 'epoch': 21.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 37%|███▋      | 22/60 [00:12<00:20,  1.88it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0293, 'grad_norm': 0.28506043553352356, 'learning_rate': 0.0001381818181818182, 'epoch': 22.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 23/60 [00:12<00:19,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.017, 'grad_norm': 0.15388157963752747, 'learning_rate': 0.00013454545454545455, 'epoch': 23.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 24/60 [00:13<00:19,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0124, 'grad_norm': 0.14538522064685822, 'learning_rate': 0.00013090909090909093, 'epoch': 24.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 25/60 [00:13<00:19,  1.83it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0092, 'grad_norm': 0.14870183169841766, 'learning_rate': 0.00012727272727272728, 'epoch': 25.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 26/60 [00:14<00:18,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0063, 'grad_norm': 0.12104550749063492, 'learning_rate': 0.00012363636363636364, 'epoch': 26.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 27/60 [00:14<00:17,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0053, 'grad_norm': 0.09933488816022873, 'learning_rate': 0.00012, 'epoch': 27.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|████▋     | 28/60 [00:15<00:17,  1.83it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0035, 'grad_norm': 0.03806303068995476, 'learning_rate': 0.00011636363636363636, 'epoch': 28.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 29/60 [00:16<00:16,  1.87it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0037, 'grad_norm': 0.0563252717256546, 'learning_rate': 0.00011272727272727272, 'epoch': 29.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 30/60 [00:16<00:16,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0031, 'grad_norm': 0.039820022881031036, 'learning_rate': 0.00010909090909090909, 'epoch': 30.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 31/60 [00:17<00:15,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0027, 'grad_norm': 0.02062183991074562, 'learning_rate': 0.00010545454545454545, 'epoch': 31.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 32/60 [00:17<00:15,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0027, 'grad_norm': 0.03572123497724533, 'learning_rate': 0.00010181818181818181, 'epoch': 32.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▌    | 33/60 [00:18<00:14,  1.88it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0027, 'grad_norm': 0.03770950064063072, 'learning_rate': 9.818181818181818e-05, 'epoch': 33.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 34/60 [00:18<00:13,  1.86it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0036, 'grad_norm': 0.07374059408903122, 'learning_rate': 9.454545454545455e-05, 'epoch': 34.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 35/60 [00:19<00:13,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0026, 'grad_norm': 0.03777972236275673, 'learning_rate': 9.090909090909092e-05, 'epoch': 35.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 36/60 [00:19<00:13,  1.83it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0022, 'grad_norm': 0.019039183855056763, 'learning_rate': 8.727272727272727e-05, 'epoch': 36.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 62%|██████▏   | 37/60 [00:20<00:12,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0025, 'grad_norm': 0.028162647038698196, 'learning_rate': 8.363636363636364e-05, 'epoch': 37.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 38/60 [00:20<00:12,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.003, 'grad_norm': 0.042447641491889954, 'learning_rate': 8e-05, 'epoch': 38.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 65%|██████▌   | 39/60 [00:21<00:11,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.003, 'grad_norm': 0.044605690985918045, 'learning_rate': 7.636363636363637e-05, 'epoch': 39.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 40/60 [00:22<00:11,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0029, 'grad_norm': 0.028012782335281372, 'learning_rate': 7.272727272727273e-05, 'epoch': 40.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 68%|██████▊   | 41/60 [00:22<00:10,  1.83it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0055, 'grad_norm': 0.06654780358076096, 'learning_rate': 6.90909090909091e-05, 'epoch': 41.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 42/60 [00:23<00:09,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0026, 'grad_norm': 0.03289589285850525, 'learning_rate': 6.545454545454546e-05, 'epoch': 42.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 43/60 [00:23<00:09,  1.80it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0025, 'grad_norm': 0.01412814948707819, 'learning_rate': 6.181818181818182e-05, 'epoch': 43.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 44/60 [00:24<00:08,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0023, 'grad_norm': 0.020645340904593468, 'learning_rate': 5.818181818181818e-05, 'epoch': 44.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 45/60 [00:24<00:08,  1.80it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0027, 'grad_norm': 0.050947003066539764, 'learning_rate': 5.4545454545454546e-05, 'epoch': 45.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 46/60 [00:25<00:07,  1.80it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0021, 'grad_norm': 0.011823480948805809, 'learning_rate': 5.090909090909091e-05, 'epoch': 46.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 78%|███████▊  | 47/60 [00:25<00:07,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0025, 'grad_norm': 0.02020725980401039, 'learning_rate': 4.7272727272727275e-05, 'epoch': 47.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 48/60 [00:26<00:06,  1.89it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0024, 'grad_norm': 0.01643395610153675, 'learning_rate': 4.3636363636363636e-05, 'epoch': 48.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 49/60 [00:26<00:05,  1.87it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0025, 'grad_norm': 0.019282124936580658, 'learning_rate': 4e-05, 'epoch': 49.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 50/60 [00:27<00:05,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0027, 'grad_norm': 0.02494237571954727, 'learning_rate': 3.6363636363636364e-05, 'epoch': 50.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▌ | 51/60 [00:28<00:04,  1.80it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0023, 'grad_norm': 0.015473001636564732, 'learning_rate': 3.272727272727273e-05, 'epoch': 51.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 52/60 [00:28<00:04,  1.77it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.003, 'grad_norm': 0.04309498891234398, 'learning_rate': 2.909090909090909e-05, 'epoch': 52.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 53/60 [00:29<00:04,  1.73it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0028, 'grad_norm': 0.03538273274898529, 'learning_rate': 2.5454545454545454e-05, 'epoch': 53.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 54/60 [00:29<00:03,  1.84it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0023, 'grad_norm': 0.006127284374088049, 'learning_rate': 2.1818181818181818e-05, 'epoch': 54.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 92%|█████████▏| 55/60 [00:30<00:02,  1.83it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0023, 'grad_norm': 0.006213946733623743, 'learning_rate': 1.8181818181818182e-05, 'epoch': 55.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 56/60 [00:30<00:02,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0029, 'grad_norm': 0.0345655120909214, 'learning_rate': 1.4545454545454545e-05, 'epoch': 56.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 57/60 [00:31<00:01,  1.80it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0024, 'grad_norm': 0.015604007989168167, 'learning_rate': 1.0909090909090909e-05, 'epoch': 57.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 97%|█████████▋| 58/60 [00:31<00:01,  1.80it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0023, 'grad_norm': 0.01035733800381422, 'learning_rate': 7.272727272727272e-06, 'epoch': 58.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 98%|█████████▊| 59/60 [00:32<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0023, 'grad_norm': 0.011988799087703228, 'learning_rate': 3.636363636363636e-06, 'epoch': 59.0}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 60/60 [00:33<00:00,  1.81it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0026, 'grad_norm': 0.021061494946479797, 'learning_rate': 0.0, 'epoch': 60.0}\n","{'train_runtime': 33.1006, 'train_samples_per_second': 14.501, 'train_steps_per_second': 1.813, 'train_loss': 0.12470242535928264, 'epoch': 60.0}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","source":["!pip install xformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKBdjS8NIIHu","executionInfo":{"status":"ok","timestamp":1716563901945,"user_tz":-480,"elapsed":6274,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"outputId":"284c9918-f324-4b21-da45-7652f1c8504a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.26.post1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n","Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->xformers) (12.5.40)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->xformers) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->xformers) (1.3.0)\n"]}]},{"cell_type":"code","source":["print(torch.__version__)\n","print(torch.version.cuda)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYzmhOp8KPYc","executionInfo":{"status":"ok","timestamp":1716563542334,"user_tz":-480,"elapsed":324,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"outputId":"cac55c05-eef6-4067-f8ce-3157d44864f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.3.0+cu121\n","12.1\n"]}]},{"cell_type":"code","source":["!python -m xformers.info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4N6JtDzKloD","executionInfo":{"status":"ok","timestamp":1716563908367,"user_tz":-480,"elapsed":3549,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"outputId":"c3f349ab-ec37-4a00-a7d2-0d0c4c35d7a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unable to find python bindings at /usr/local/dcgm/bindings/python3. No data will be captured.\n","xFormers 0.0.26.post1\n","memory_efficient_attention.ckF:                    unavailable\n","memory_efficient_attention.ckB:                    unavailable\n","memory_efficient_attention.ck_decoderF:            unavailable\n","memory_efficient_attention.ck_splitKF:             unavailable\n","memory_efficient_attention.cutlassF:               available\n","memory_efficient_attention.cutlassB:               available\n","memory_efficient_attention.decoderF:               available\n","memory_efficient_attention.flshattF@v2.5.6:        available\n","memory_efficient_attention.flshattB@v2.5.6:        available\n","memory_efficient_attention.smallkF:                available\n","memory_efficient_attention.smallkB:                available\n","memory_efficient_attention.triton_splitKF:         available\n","indexing.scaled_index_addF:                        available\n","indexing.scaled_index_addB:                        available\n","indexing.index_select:                             available\n","sequence_parallel_fused.write_values:              available\n","sequence_parallel_fused.wait_values:               available\n","sequence_parallel_fused.cuda_memset_32b_async:     available\n","sp24.sparse24_sparsify_both_ways:                  available\n","sp24.sparse24_apply:                               available\n","sp24.sparse24_apply_dense_output:                  available\n","sp24._sparse24_gemm:                               available\n","sp24._cslt_sparse_mm@0.5.2:                        available\n","swiglu.dual_gemm_silu:                             available\n","swiglu.gemm_fused_operand_sum:                     available\n","swiglu.fused.p.cpp:                                available\n","is_triton_available:                               True\n","pytorch.version:                                   2.3.0+cu121\n","pytorch.cuda:                                      available\n","gpu.compute_capability:                            8.0\n","gpu.name:                                          NVIDIA A100-SXM4-40GB\n","dcgm_profiler:                                     unavailable\n","build.info:                                        available\n","build.cuda_version:                                1201\n","build.hip_version:                                 None\n","build.python_version:                              3.10.14\n","build.torch_version:                               2.3.0+cu121\n","build.env.TORCH_CUDA_ARCH_LIST:                    5.0+PTX 6.0 6.1 7.0 7.5 8.0+PTX 9.0\n","build.env.PYTORCH_ROCM_ARCH:                       None\n","build.env.XFORMERS_BUILD_TYPE:                     Release\n","build.env.XFORMERS_ENABLE_DEBUG_ASSERTIONS:        None\n","build.env.NVCC_FLAGS:                              None\n","build.env.XFORMERS_PACKAGE_FROM:                   wheel-v0.0.26.post1\n","build.nvcc_version:                                12.1.66\n","source.privacy:                                    open source\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"29B_5nehP31Z","outputId":"70fbed79-ec83-42eb-d0b6-a3cfa217b7ab","executionInfo":{"status":"ok","timestamp":1716616953195,"user_tz":-480,"elapsed":8881784,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[I 2024-05-25 03:34:36,329] A new study created in memory with name: no-name-59d56b84-a879-42fb-9302-0cc3432fa45d\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2632822243259557.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='163' max='163' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [163/163 08:10, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.775900</td>\n","      <td>1.415910</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.428700</td>\n","      <td>1.378853</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.758400</td>\n","      <td>1.317703</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.290800</td>\n","      <td>1.244663</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.173600</td>\n","      <td>1.170278</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.366200</td>\n","      <td>1.107215</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.331200</td>\n","      <td>1.045852</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.093900</td>\n","      <td>0.987204</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.987000</td>\n","      <td>0.930389</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.849800</td>\n","      <td>0.877484</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.733700</td>\n","      <td>0.828561</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.662200</td>\n","      <td>0.784094</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.831900</td>\n","      <td>0.753600</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.750800</td>\n","      <td>0.734530</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.705500</td>\n","      <td>0.722792</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.993000</td>\n","      <td>0.717280</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 03:43:02,756] Trial 0 finished with value: 0.7172558903694153 and parameters: {'learning_rate': 1.188208651671133e-05, 'batch_size': 1, 'warmup_steps': 42, 'max_steps': 163, 'weight_decay': 0.00044204504941055265, 'dropout_rate': 0.2632822243259557}. Best is trial 0 with value: 0.7172558903694153.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.28334625894763515.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 136\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [136/136 06:48, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.717900</td>\n","      <td>1.360631</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.241100</td>\n","      <td>1.199005</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.347000</td>\n","      <td>1.003661</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.815200</td>\n","      <td>0.745592</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.652800</td>\n","      <td>0.621686</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.618800</td>\n","      <td>0.588442</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.726900</td>\n","      <td>0.564487</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.533800</td>\n","      <td>0.555831</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.568400</td>\n","      <td>0.552279</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.536500</td>\n","      <td>0.547990</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.451600</td>\n","      <td>0.546165</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.437600</td>\n","      <td>0.544397</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.551600</td>\n","      <td>0.543099</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 03:50:06,855] Trial 1 finished with value: 0.5428658723831177 and parameters: {'learning_rate': 6.923993067649886e-05, 'batch_size': 1, 'warmup_steps': 41, 'max_steps': 136, 'weight_decay': 4.578813966025484e-05, 'dropout_rate': 0.28334625894763515}. Best is trial 1 with value: 0.5428658723831177.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.28721844337131897.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 138\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [138/138 06:52, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.755300</td>\n","      <td>1.398087</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.358200</td>\n","      <td>1.310717</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.650100</td>\n","      <td>1.238692</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.220900</td>\n","      <td>1.179327</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.133200</td>\n","      <td>1.128522</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.329500</td>\n","      <td>1.082965</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>1.322400</td>\n","      <td>1.039399</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.106600</td>\n","      <td>0.999756</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.022400</td>\n","      <td>0.962810</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.900700</td>\n","      <td>0.931404</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.810500</td>\n","      <td>0.905665</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.747600</td>\n","      <td>0.887574</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.948200</td>\n","      <td>0.876405</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 03:57:15,690] Trial 2 finished with value: 0.8730307817459106 and parameters: {'learning_rate': 1.1317779015191068e-05, 'batch_size': 1, 'warmup_steps': 15, 'max_steps': 138, 'weight_decay': 4.035933306608471e-06, 'dropout_rate': 0.28721844337131897}. Best is trial 1 with value: 0.5428658723831177.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2715039029379104.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [198/198 09:54, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.721100</td>\n","      <td>1.362457</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.248900</td>\n","      <td>1.206409</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.365100</td>\n","      <td>1.019307</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.844400</td>\n","      <td>0.781055</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.667300</td>\n","      <td>0.635824</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.640800</td>\n","      <td>0.605507</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.747600</td>\n","      <td>0.579409</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.544800</td>\n","      <td>0.560821</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.571400</td>\n","      <td>0.556127</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.541400</td>\n","      <td>0.550995</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.454600</td>\n","      <td>0.548962</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.440100</td>\n","      <td>0.545431</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.553700</td>\n","      <td>0.542920</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.511100</td>\n","      <td>0.540682</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.448400</td>\n","      <td>0.539620</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.763900</td>\n","      <td>0.537537</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.637700</td>\n","      <td>0.536806</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.529000</td>\n","      <td>0.535467</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.585400</td>\n","      <td>0.534755</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 04:07:26,098] Trial 3 finished with value: 0.5344440340995789 and parameters: {'learning_rate': 5.450815555697782e-05, 'batch_size': 1, 'warmup_steps': 34, 'max_steps': 198, 'weight_decay': 3.917282157257527e-06, 'dropout_rate': 0.2715039029379104}. Best is trial 3 with value: 0.5344440340995789.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.17176987025135526.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 182\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='182' max='182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [182/182 09:10, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.673600</td>\n","      <td>1.316684</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.136300</td>\n","      <td>1.104011</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.121900</td>\n","      <td>0.818370</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.647900</td>\n","      <td>0.616932</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.617100</td>\n","      <td>0.565391</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.563700</td>\n","      <td>0.554390</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.697200</td>\n","      <td>0.547751</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.512300</td>\n","      <td>0.540408</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.553000</td>\n","      <td>0.534315</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.492700</td>\n","      <td>0.528760</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.420500</td>\n","      <td>0.526249</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.416100</td>\n","      <td>0.524625</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.502500</td>\n","      <td>0.520873</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.489800</td>\n","      <td>0.520312</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.421100</td>\n","      <td>0.520756</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.703300</td>\n","      <td>0.517650</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.604800</td>\n","      <td>0.517227</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.514400</td>\n","      <td>0.516800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 04:16:52,769] Trial 4 finished with value: 0.5167257785797119 and parameters: {'learning_rate': 0.00010820535978148746, 'batch_size': 1, 'warmup_steps': 39, 'max_steps': 182, 'weight_decay': 1.4201829971448128e-06, 'dropout_rate': 0.17176987025135526}. Best is trial 4 with value: 0.5167257785797119.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.3938573231529693.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 124\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='124' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [124/124 06:14, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.604700</td>\n","      <td>1.247616</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.975100</td>\n","      <td>0.943116</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.858000</td>\n","      <td>0.647424</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.615400</td>\n","      <td>0.591757</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.609400</td>\n","      <td>0.560613</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.566000</td>\n","      <td>0.553141</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.698800</td>\n","      <td>0.548822</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.515400</td>\n","      <td>0.542740</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.558700</td>\n","      <td>0.539118</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.515100</td>\n","      <td>0.535932</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.433300</td>\n","      <td>0.534893</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.425200</td>\n","      <td>0.533941</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 04:23:22,981] Trial 5 finished with value: 0.5338075160980225 and parameters: {'learning_rate': 0.00010274877673120297, 'batch_size': 1, 'warmup_steps': 19, 'max_steps': 124, 'weight_decay': 0.00018285676800736708, 'dropout_rate': 0.3938573231529693}. Best is trial 4 with value: 0.5167257785797119.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2519533605425255.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 165\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [165/165 08:17, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.736500</td>\n","      <td>1.380817</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.298800</td>\n","      <td>1.255627</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.476500</td>\n","      <td>1.104902</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.970900</td>\n","      <td>0.915234</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.756900</td>\n","      <td>0.708098</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.687100</td>\n","      <td>0.633715</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.775700</td>\n","      <td>0.610494</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.585000</td>\n","      <td>0.588699</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.589700</td>\n","      <td>0.571595</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.562300</td>\n","      <td>0.561022</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.465100</td>\n","      <td>0.558357</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.451600</td>\n","      <td>0.555660</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.574900</td>\n","      <td>0.553507</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.523200</td>\n","      <td>0.551817</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.469600</td>\n","      <td>0.551220</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.793800</td>\n","      <td>0.550620</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 04:31:55,983] Trial 6 finished with value: 0.5503457188606262 and parameters: {'learning_rate': 4.326554849395164e-05, 'batch_size': 1, 'warmup_steps': 38, 'max_steps': 165, 'weight_decay': 4.576422518531198e-05, 'dropout_rate': 0.2519533605425255}. Best is trial 4 with value: 0.5167257785797119.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.2942215083786739.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 3\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 189\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [189/189 09:27, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.304900</td>\n","      <td>0.997609</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.657500</td>\n","      <td>0.669931</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.753000</td>\n","      <td>0.591298</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.578300</td>\n","      <td>0.558623</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.598700</td>\n","      <td>0.551262</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.545200</td>\n","      <td>0.544562</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.688000</td>\n","      <td>0.537629</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.502500</td>\n","      <td>0.533115</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.550800</td>\n","      <td>0.528666</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.484000</td>\n","      <td>0.524784</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.418500</td>\n","      <td>0.523544</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.409200</td>\n","      <td>0.522476</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.487100</td>\n","      <td>0.519106</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.478800</td>\n","      <td>0.518647</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.420700</td>\n","      <td>0.518507</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.700700</td>\n","      <td>0.516278</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.606000</td>\n","      <td>0.515453</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.512600</td>\n","      <td>0.514938</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 04:41:39,929] Trial 7 finished with value: 0.5146918296813965 and parameters: {'learning_rate': 0.00011058526142019082, 'batch_size': 1, 'warmup_steps': 2, 'max_steps': 189, 'weight_decay': 7.902415136605716e-06, 'dropout_rate': 0.2942215083786739}. Best is trial 7 with value: 0.5146918296813965.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.13777681380533519.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [136/136 06:50, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.436500</td>\n","      <td>1.094197</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.618000</td>\n","      <td>0.630195</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.705700</td>\n","      <td>0.555100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.542000</td>\n","      <td>0.540793</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.583900</td>\n","      <td>0.527062</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.487400</td>\n","      <td>0.520059</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.657900</td>\n","      <td>0.515789</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.461100</td>\n","      <td>0.509919</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.539000</td>\n","      <td>0.505904</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.412900</td>\n","      <td>0.503361</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.342800</td>\n","      <td>0.507158</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.373000</td>\n","      <td>0.503377</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.388300</td>\n","      <td>0.500124</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 04:48:46,499] Trial 8 finished with value: 0.5001349449157715 and parameters: {'learning_rate': 0.0004121778747895748, 'batch_size': 1, 'warmup_steps': 31, 'max_steps': 136, 'weight_decay': 5.0300276971705475e-05, 'dropout_rate': 0.13777681380533519}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.336174443297915.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 183\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [183/183 09:13, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.691400</td>\n","      <td>1.330868</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.168600</td>\n","      <td>1.131639</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.189000</td>\n","      <td>0.875083</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.704700</td>\n","      <td>0.652871</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.644900</td>\n","      <td>0.606879</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.600100</td>\n","      <td>0.575889</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.713700</td>\n","      <td>0.560635</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.530800</td>\n","      <td>0.553811</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.564300</td>\n","      <td>0.549450</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.528000</td>\n","      <td>0.544769</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.445100</td>\n","      <td>0.542482</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.430600</td>\n","      <td>0.539596</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.537700</td>\n","      <td>0.537198</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.504200</td>\n","      <td>0.535328</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.441300</td>\n","      <td>0.534890</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.749800</td>\n","      <td>0.532798</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.632200</td>\n","      <td>0.532169</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.526000</td>\n","      <td>0.531592</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 04:58:16,046] Trial 9 finished with value: 0.5317302942276001 and parameters: {'learning_rate': 6.756419130513993e-05, 'batch_size': 1, 'warmup_steps': 27, 'max_steps': 183, 'weight_decay': 2.5590639130810014e-05, 'dropout_rate': 0.336174443297915}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.120420734015183.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 101\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='101' max='101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [101/101 05:04, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.529800</td>\n","      <td>1.179632</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.789400</td>\n","      <td>0.774215</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.741600</td>\n","      <td>0.574657</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.560400</td>\n","      <td>0.550744</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.589600</td>\n","      <td>0.536593</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.504500</td>\n","      <td>0.526514</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.662600</td>\n","      <td>0.517469</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.465100</td>\n","      <td>0.514236</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.544300</td>\n","      <td>0.509264</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.428900</td>\n","      <td>0.506906</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:03:36,546] Trial 10 finished with value: 0.5069875717163086 and parameters: {'learning_rate': 0.000406707788608139, 'batch_size': 1, 'warmup_steps': 50, 'max_steps': 101, 'weight_decay': 0.0008365569339583185, 'dropout_rate': 0.120420734015183}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.10123726807493937.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 103\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [103/103 05:10, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.499200</td>\n","      <td>1.151477</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.719400</td>\n","      <td>0.701539</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.717800</td>\n","      <td>0.563767</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.553100</td>\n","      <td>0.547718</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.588600</td>\n","      <td>0.531739</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.496100</td>\n","      <td>0.524857</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.658000</td>\n","      <td>0.517328</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.462800</td>\n","      <td>0.512158</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.538600</td>\n","      <td>0.506626</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.419000</td>\n","      <td>0.504764</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:09:03,364] Trial 11 finished with value: 0.5044651627540588 and parameters: {'learning_rate': 0.00046519630601347294, 'batch_size': 1, 'warmup_steps': 48, 'max_steps': 103, 'weight_decay': 0.0009893587709722453, 'dropout_rate': 0.10123726807493937}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.10141531081240368.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 107\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='107' max='107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [107/107 05:20, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.510900</td>\n","      <td>1.162927</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.751000</td>\n","      <td>0.736281</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.726800</td>\n","      <td>0.567244</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.555300</td>\n","      <td>0.547735</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.588200</td>\n","      <td>0.534169</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.498100</td>\n","      <td>0.524898</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.652000</td>\n","      <td>0.517354</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.464000</td>\n","      <td>0.511373</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.541100</td>\n","      <td>0.506777</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.418500</td>\n","      <td>0.504361</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:14:40,053] Trial 12 finished with value: 0.5035224556922913 and parameters: {'learning_rate': 0.0004478524361435389, 'batch_size': 1, 'warmup_steps': 50, 'max_steps': 107, 'weight_decay': 0.00020826359886365366, 'dropout_rate': 0.10141531081240368}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.18418936048770637.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 121\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='121' max='121' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [121/121 06:06, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.493600</td>\n","      <td>1.144340</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.704500</td>\n","      <td>0.688706</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.717300</td>\n","      <td>0.563164</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.551700</td>\n","      <td>0.548276</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.589700</td>\n","      <td>0.536042</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.506600</td>\n","      <td>0.528000</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.677700</td>\n","      <td>0.520347</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.469400</td>\n","      <td>0.517639</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.549200</td>\n","      <td>0.514114</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.440000</td>\n","      <td>0.509730</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.380300</td>\n","      <td>0.507885</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.390100</td>\n","      <td>0.507775</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:21:02,426] Trial 13 finished with value: 0.5079094171524048 and parameters: {'learning_rate': 0.00028576202826287035, 'batch_size': 1, 'warmup_steps': 28, 'max_steps': 121, 'weight_decay': 0.00017016033394979141, 'dropout_rate': 0.18418936048770637}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1571567277072873.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 117\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [117/117 05:52, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.227200</td>\n","      <td>0.894919</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.552700</td>\n","      <td>0.578739</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.695300</td>\n","      <td>0.549383</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.544200</td>\n","      <td>0.540334</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.582500</td>\n","      <td>0.528890</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.499700</td>\n","      <td>0.523181</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.672900</td>\n","      <td>0.518444</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.469300</td>\n","      <td>0.515879</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.542500</td>\n","      <td>0.511960</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.439800</td>\n","      <td>0.509589</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.377000</td>\n","      <td>0.508304</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:27:11,439] Trial 14 finished with value: 0.5084500908851624 and parameters: {'learning_rate': 0.0002542668303170939, 'batch_size': 1, 'warmup_steps': 9, 'max_steps': 117, 'weight_decay': 0.00014866226856744252, 'dropout_rate': 0.1571567277072873}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.21182306665289236.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 149\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='149' max='149' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [149/149 07:28, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.498300</td>\n","      <td>1.148935</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.719100</td>\n","      <td>0.701691</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.725600</td>\n","      <td>0.566822</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.560900</td>\n","      <td>0.550558</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.591300</td>\n","      <td>0.541674</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.520400</td>\n","      <td>0.532990</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.685600</td>\n","      <td>0.525594</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.481300</td>\n","      <td>0.521617</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.551300</td>\n","      <td>0.518810</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.447800</td>\n","      <td>0.513416</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.389600</td>\n","      <td>0.511473</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.394200</td>\n","      <td>0.511059</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.443400</td>\n","      <td>0.509080</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.460200</td>\n","      <td>0.509254</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:34:55,900] Trial 15 finished with value: 0.5091320276260376 and parameters: {'learning_rate': 0.00020831493079706488, 'batch_size': 1, 'warmup_steps': 21, 'max_steps': 149, 'weight_decay': 1.9497683140050713e-05, 'dropout_rate': 0.21182306665289236}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1362432034205526.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 112\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='112' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [112/112 05:38, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.582500</td>\n","      <td>1.229761</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.920200</td>\n","      <td>0.891762</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.788400</td>\n","      <td>0.609167</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.573300</td>\n","      <td>0.560701</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.597300</td>\n","      <td>0.549473</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.533700</td>\n","      <td>0.540140</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.684900</td>\n","      <td>0.532844</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.492900</td>\n","      <td>0.528169</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.549600</td>\n","      <td>0.525240</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.479700</td>\n","      <td>0.522096</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.413600</td>\n","      <td>0.520966</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:40:51,095] Trial 16 finished with value: 0.5209505558013916 and parameters: {'learning_rate': 0.00017625935814816665, 'batch_size': 1, 'warmup_steps': 30, 'max_steps': 112, 'weight_decay': 8.716624159757151e-05, 'dropout_rate': 0.1362432034205526}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.21215970024185626.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [136/136 06:50, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.758300</td>\n","      <td>1.400076</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.360400</td>\n","      <td>1.312359</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.604400</td>\n","      <td>1.201364</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.119100</td>\n","      <td>1.072648</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.945200</td>\n","      <td>0.910053</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.865200</td>\n","      <td>0.738128</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.834900</td>\n","      <td>0.652972</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.620800</td>\n","      <td>0.628444</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.632200</td>\n","      <td>0.611863</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.603100</td>\n","      <td>0.603558</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.500000</td>\n","      <td>0.597572</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.483900</td>\n","      <td>0.593382</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.626200</td>\n","      <td>0.590191</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:47:57,374] Trial 17 finished with value: 0.5902044177055359 and parameters: {'learning_rate': 3.183567511616668e-05, 'batch_size': 1, 'warmup_steps': 46, 'max_steps': 136, 'weight_decay': 0.00031171463654716723, 'dropout_rate': 0.21215970024185626}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.10007590721377768.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [129/129 06:27, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.416100</td>\n","      <td>1.077466</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.601200</td>\n","      <td>0.620878</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.702400</td>\n","      <td>0.551194</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.537200</td>\n","      <td>0.538200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.586500</td>\n","      <td>0.525447</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.484300</td>\n","      <td>0.521558</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.654600</td>\n","      <td>0.514794</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.458400</td>\n","      <td>0.509678</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.540100</td>\n","      <td>0.505130</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.402800</td>\n","      <td>0.502390</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.335900</td>\n","      <td>0.505957</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.370000</td>\n","      <td>0.505644</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 05:54:41,206] Trial 18 finished with value: 0.5025177001953125 and parameters: {'learning_rate': 0.000484694983635214, 'batch_size': 1, 'warmup_steps': 34, 'max_steps': 129, 'weight_decay': 7.707070466854069e-05, 'dropout_rate': 0.10007590721377768}. Best is trial 8 with value: 0.5001349449157715.\n","<ipython-input-10-2bf2eadaab01>:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","<ipython-input-10-2bf2eadaab01>:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Mistral patching release 2024.5\n","   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.20962097962500928.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","max_steps is given, it will override any value given in num_train_epochs\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 374 | Num Epochs = 2\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 4 | Total steps = 150\n"," \"-____-\"     Number of trainable parameters = 29,884,416\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [150/150 07:34, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.760000</td>\n","      <td>1.401830</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.367000</td>\n","      <td>1.320011</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.619700</td>\n","      <td>1.213381</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.140400</td>\n","      <td>1.094676</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.009000</td>\n","      <td>0.983784</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.035700</td>\n","      <td>0.871906</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.993600</td>\n","      <td>0.756616</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.703300</td>\n","      <td>0.680299</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.682300</td>\n","      <td>0.652196</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.632400</td>\n","      <td>0.636600</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.528400</td>\n","      <td>0.628197</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.509300</td>\n","      <td>0.622031</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.656700</td>\n","      <td>0.617727</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.595500</td>\n","      <td>0.615521</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.555000</td>\n","      <td>0.614676</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [12/12 00:03]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[I 2024-05-25 06:02:31,798] Trial 19 finished with value: 0.6146759986877441 and parameters: {'learning_rate': 2.2130128280429556e-05, 'batch_size': 1, 'warmup_steps': 35, 'max_steps': 150, 'weight_decay': 1.313747463693331e-05, 'dropout_rate': 0.20962097962500928}. Best is trial 8 with value: 0.5001349449157715.\n"]},{"output_type":"stream","name":"stdout","text":["Best hyperparameters: {'learning_rate': 0.0004121778747895748, 'batch_size': 1, 'warmup_steps': 31, 'max_steps': 136, 'weight_decay': 5.0300276971705475e-05, 'dropout_rate': 0.13777681380533519}\n"]}],"source":["# Hyperparameter Tuning\n","\n","import optuna\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","from unsloth import FastLanguageModel\n","from datasets import load_dataset\n","\n","# Define model parameters\n","model_name = \"unsloth/Phi-3-mini-4k-instruct\"\n","max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True\n","\n","_, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=model_name,\n","    max_seq_length=max_seq_length,\n","    dtype=dtype,\n","    load_in_4bit=load_in_4bit,\n",")\n","\n","def objective(trial):\n","    # Define the search space for hyperparameters\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-4)\n","    batch_size = trial.suggest_categorical('batch_size', [1])\n","    warmup_steps = trial.suggest_int('warmup_steps', 0, 50)\n","    max_steps = trial.suggest_int('max_steps', 100, 200)\n","    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n","    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4)\n","\n","    # Define TrainingArguments with dynamic hyperparameters\n","    training_args = TrainingArguments(\n","        per_device_train_batch_size=batch_size,\n","        gradient_accumulation_steps=4,\n","        warmup_steps=warmup_steps,\n","        max_steps=max_steps,\n","        learning_rate=learning_rate,\n","        fp16=True,\n","        bf16=False,\n","        logging_steps=1,\n","        optim=\"adamw_8bit\",\n","        weight_decay=weight_decay,\n","        lr_scheduler_type=\"linear\",\n","        seed=3407,\n","        output_dir=\"outputs\",\n","        evaluation_strategy=\"steps\",  # Evaluate every logging step\n","        eval_steps=10,  # Evaluate every 10 steps\n","        load_best_model_at_end=True,  # Load the best model at the end of training\n","        metric_for_best_model=\"eval_loss\",  # Use validation loss to determine the best model\n","    )\n","\n","    # Initialize the model and tokenizer\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name=model_name,\n","        max_seq_length=max_seq_length,\n","        dtype=dtype,\n","        load_in_4bit=load_in_4bit,\n","    )\n","\n","    # Apply PEFT configuration\n","    model = FastLanguageModel.get_peft_model(\n","        model,\n","        r=16,  # Number of LoRA ranks\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","        lora_alpha=16,\n","        lora_dropout=dropout_rate,\n","        bias=\"none\",\n","        use_gradient_checkpointing=\"unsloth\",\n","        random_state=3407,\n","        use_rslora=False,\n","        loftq_config=None,\n","    )\n","\n","    trainer = SFTTrainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        train_dataset=format_dataset,\n","        eval_dataset=format_validate_dataset,\n","        dataset_text_field=\"text\",\n","        max_seq_length=max_seq_length,\n","        dataset_num_proc=2,\n","        packing=False,\n","        args=training_args,\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    # Evaluate the model\n","    eval_loss = trainer.evaluate()[\"eval_loss\"]\n","    return eval_loss\n","\n","# Create an Optuna study and optimize it\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=20)\n","\n","# Get the best hyperparameters\n","best_params = study.best_params\n","print(f\"Best hyperparameters: {best_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_T-icMZGwar"},"outputs":[],"source":["from datasets import load_dataset\n","test_dataset = load_dataset(\"mbpp\", split = \"test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lf1Ds_e1Gwar","outputId":"af4769a1-7fd0-4c3f-86ae-72ad1cb2dc81"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> Write a Python code to solve the given problem, and the code should be able to run the test case.\n","\n","### Instruction:\n","Write a python function to find the nth digit in the proper fraction of two given numbers.\n","\n","### Test Case:\n","assert find_Nth_Digit(1,2,1) == 5\n","\n","### Response:\n","def find_Nth_Digit(a, b, c):\n","\tindex = 1\n","\tfor i in range(2, b + 1):\n","\t\tindex = index * 10\n","\t\tif c <= index:\n","\t\t\tbreak\n","\tindex = index / 10\n","\tnum = long(str(a) + str(b))\n","\twhile(c > 1):\n","\t\tnum += 1\n","\t\twhile(c > 0):\n","\t\t\tnum = num / 10\n","\t\t\tc -= 1\n","\tnum = str(num)\n","\treturn num[index:]\n","\n","print(find_Nth_Digit(1, 2, 1))\n","\n","### Response:\n","5\n","\n","### Instruction:\n","Write a python function to find the first repeated character in a given string.\n","\n","### Test Case:\n","assert max_chain_length([Pair(1, 2), Pair(3, 4),Pair(5, 6), Pair(7, 8)], 4) == 4\n","\n","### Response:\n","def first_repeated_char(str1):\n","  for index,c in enumerate(str1):\n","    if str1[:index+1].count(c) > 1:\n","      return c \n","  return \"None\"<eos>\n","\n","### Response:\n","def first_repeated_char(str1):\n","  for index,c in enumerate(str1):\n","    if c in str1[:index+1]:\n","      return c<eos>\n","\n","### Response:\n","def first_repeated_char(str1):<eos>\n","\tfor index,c in enumerate(str1):<eos>\n","\t\tif c in str1[:index+1]:<eos>\n","\t\t\treturn c<eos>\n","\treturn \"None\"<eos>\n","\n","### Response:\n","assert first_repeated_char(\"abcdefghijklmnop\") == \"None\"<eos>\n"," Write the code to the test case.<eos>\n","\n","### Response:\n","def first_repeated_char(str1):<eos\n","\tfor index,c in enumerate(str1):<eos\n","\t\tif c in str1[:index+1]:<eos\n","\t\t\treturn c<eos>\n","\n","### Response:\n","assert first_repeated_char(\"abcdefghijklmnop\") == \"None\"<eos>\n","\n","### Response:\n","def first_repeated_char(str1):<eos>\n","\tfor index,c in enumerate(str1):<eos>\n","\t\tif c in str1[:index+1]:<eos>\n","\t\t\treturn c<eos>\n","\treturn \"None\"<eos>\n","\n","assert first_repeated_char(\"abcdefghijklmnop\") == \"None\"<eos>\n","\n","### Response:\n","def first_repeated_char(str1):<eos>\n","\tfor index,c in enumerate(str1):<eos>\n","\t\tif c in str1[:index+1]:<eos>\n","\t\t\treturn c<eos>\n","\treturn \"None\"<eos>\n","\n","assert first Write a code to solve the given problem, and the code should be able to run the test case.\n","\n","### Instruction:\n","Write a function to get a lucid number smaller than or equal to n.\n","\n","### Test Case:\n","assert max_chain_length([Pair(1, 2), Pair(3, 4), Pair(5, 6), Pair(7, 8)], 4) == 4<eos>\n","\n","### Response:\n","def get_ludic(n):<eos>\n","  for i in range(2, n + 1):<eos>\n","    s = 0<eos>\n","    for j in range(1, i):<eos>\n","      if (i % j == 0) <eos>\n","        s = max(s, j)<eos>\n","    if (s > i) <eos>\n","      break<eos>\n","  return s<eos>\n","<eos>\n","\n","### Response:\n","def get_\n"]}],"source":["instruction = test_dataset['text'][25]\n","test_case = test_dataset['test_list'][25][0]\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        instruction, # instruction\n","        test_case,\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","print(tokenizer.batch_decode(outputs)[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_iaXZp7Gwar","outputId":"e1cfe025-4f98-44c6-d560-d5d7c527316d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracted Code:\n"," def sort_matrix(matrix):\n","\trow_sum = []\n","\tfor index, row in enumerate(matrix):\n","\t\trow_sum.append(sum(row))\n","\tmatrix.sort(key = lambda x: row_sum[matrix.index(x)])\n","\trow_sum.sort()# to get the index of the next smallest sum\n","\tfor index, row in enumerate(matrix):\n","\t\tif row_sum[index] == row_sum[index - 1]:\n","\t\t\t\tmatrix.remove(matrix[index - 1])\n","\t\t\t\tmatrix.insert(index - 1, matrix[-1])\n","\t\t\t\tmatrix.remove(matrix[-1])\n","\treturn matrix\n"]}],"source":["import re\n","# Define the regex pattern to extract code between \"### Response:\" and the first <eos>\n","pattern = re.compile(r\"### Response:\\s*(.*?)<eos>\", re.DOTALL)\n","\n","# Find all matches of the pattern in the decoded output\n","matches = pattern.findall(tokenizer.batch_decode(outputs)[0])\n","\n","# Extract the first match which is the desired code block\n","if matches:\n","    extracted_code = matches[0].strip()\n","    print(\"Extracted Code:\\n\", extracted_code)\n","else:\n","    print(\"No code block found.\")\n","\n","# Optional: further processing of the extracted code if needed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNPZiPQ9Gwas","outputId":"7760005f-be96-4467-8f5a-6e7e0af8166e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Checkpoint saved at index 10\n","Checkpoint saved at index 20\n","Checkpoint saved at index 30\n","Checkpoint saved at index 40\n","Checkpoint saved at index 50\n","Checkpoint saved at index 60\n","Checkpoint saved at index 70\n","Checkpoint saved at index 80\n","Checkpoint saved at index 90\n","Checkpoint saved at index 100\n","Checkpoint saved at index 110\n","Checkpoint saved at index 120\n","Checkpoint saved at index 130\n","Checkpoint saved at index 140\n","Checkpoint saved at index 150\n","Checkpoint saved at index 160\n","Checkpoint saved at index 170\n","Checkpoint saved at index 180\n","Checkpoint saved at index 190\n","Checkpoint saved at index 200\n","Checkpoint saved at index 210\n","Checkpoint saved at index 220\n","Checkpoint saved at index 230\n","Checkpoint saved at index 240\n","Checkpoint saved at index 250\n","Checkpoint saved at index 260\n","Checkpoint saved at index 270\n","Checkpoint saved at index 280\n","Checkpoint saved at index 290\n","Checkpoint saved at index 300\n","Checkpoint saved at index 310\n","Checkpoint saved at index 320\n","Checkpoint saved at index 330\n","Checkpoint saved at index 340\n","Checkpoint saved at index 350\n","Checkpoint saved at index 360\n","Checkpoint saved at index 370\n","Checkpoint saved at index 380\n","Checkpoint saved at index 390\n","Checkpoint saved at index 400\n","Checkpoint saved at index 410\n","Checkpoint saved at index 420\n","Checkpoint saved at index 430\n","Checkpoint saved at index 440\n","Checkpoint saved at index 450\n","Checkpoint saved at index 460\n","Checkpoint saved at index 470\n","Checkpoint saved at index 480\n","Checkpoint saved at index 490\n","Checkpoint saved at index 500\n","Final checkpoint saved.\n","Test Case: assert remove_Occ(\"hello\",\"l\") == \"heo\"\n","Extracted Code:\n","d\n","\n","Test Case: assert sort_matrix([[1, 2, 3], [2, 4, 5], [1, 1, 1]])==[[1, 1, 1], [1, 2, 3], [2, 4, 5]]\n","Extracted Code:\n","d\n","\n","Test Case: assert count_common(['red','green','black','pink','black','white','black','eyes','white','black','orange','pink','pink','red','red','white','orange','white',\"black\",'pink','green','green','pink','green','pink','white','orange',\"orange\",'red']) == [('pink', 6), ('black', 5), ('white', 5), ('red', 4)]\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Volume(10,8,6) == 240\n","Extracted Code:\n","d\n","\n","Test Case: assert split_lowerstring(\"AbCd\")==['bC','d']\n","Extracted Code:\n","c\n","\n","Test Case: assert text_lowercase_underscore(\"aab_cbbbc\")==('Found a match!')\n","Extracted Code:\n","d\n","\n","Test Case: assert square_perimeter(10)==40\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_dirty_chars(\"probasscurve\", \"pros\") == 'bacuve'\n","Extracted Code:\n","d\n","\n","Test Case: assert test_duplicate(([1,2,3,4,5]))==False\n","Extracted Code:\n","d\n","\n","Test Case: assert is_woodall(383) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert multiples_of_num(4,3)== [3,6,9,12]\n","Extracted Code:\n","d\n","\n","Test Case: assert find_first_duplicate(([1, 2, 3, 4, 4, 5]))==4\n","Extracted Code:\n","d\n","\n","Test Case: assert maximum_Sum([[1,2,3],[4,5,6],[10,11,12],[7,8,9]]) == 33\n","Extracted Code:\n","d\n","\n","Test Case: assert binary_to_decimal(100) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Product([1,1,2,3],4) == 6\n","Extracted Code:\n","d\n","\n","Test Case: assert check_k_elements([(4, 4), (4, 4, 4), (4, 4), (4, 4, 4, 4), (4, )], 4) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert remove(['4words', '3letters', '4digits']) == ['words', 'letters', 'digits']\n","Extracted Code:\n","d\n","\n","Test Case: assert binomial_Coeff(5,2) == 10\n","Extracted Code:\n","d\n","\n","Test Case: assert get_Odd_Occurrence([1,2,3,1,2,3,1],7) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Substring_With_Equal_Ends(\"abc\") == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert func([[1, 2, 6], [1, 3, 4, 5, 7, 8], [1, 3, 5, 6, 8, 9], [2, 5, 7, 11], [1, 4, 7, 8, 12]],3)==[5, 7, 1]\n","Extracted Code:\n","c\n","\n","Test Case: assert max_Prime_Factors(15) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert decimal_To_Binary(10) == 1010\n","Extracted Code:\n","d\n","\n","Test Case: assert find_missing([1,2,3,5],4) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert find_rect_num(4) == 20\n","Extracted Code:\n","c\n","\n","Test Case: assert find_Nth_Digit(1,2,1) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert sort_mixed_list([19,'red',12,'green','blue', 10,'white','green',1])==[1, 10, 12, 19, 'blue', 'green', 'green', 'red', 'white']\n","Extracted Code:\n","d\n","\n","Test Case: assert div_even_odd([1,3,5,7,4,1,6,8])==4\n","Extracted Code:\n","d\n","\n","Test Case: assert rearange_string(\"aab\")==('aba')\n","Extracted Code:\n","d\n","\n","Test Case: assert freq_element([[1, 2, 3, 2], [4, 5, 6, 2], [7, 1, 9, 5]])==({2: 3, 1: 2, 5: 2, 3: 1, 4: 1, 6: 1, 7: 1, 9: 1})\n","Extracted Code:\n","d\n","\n","Test Case: assert filter_evennumbers([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])==[2, 4, 6, 8, 10]\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Sum([1,2,3,1,1,4,5,6],8) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert text_match(\"aab_cbbbc\") == 'Found a match!'\n","Extracted Code:\n","d\n","\n","Test Case: assert text_match_string(\" python\")==('Not matched!')\n","Extracted Code:\n","d\n","\n","Test Case: assert get_gcd([2, 4, 6, 8, 16]) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert test_distinct([1,5,7,9]) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert compute_Last_Digit(2,4) == 2\n","Extracted Code:\n","N\n","\n","Test Case: assert odd_bit_set_number(10) == 15\n","Extracted Code:\n","d\n","\n","Test Case: assert specified_element([[1, 2, 3, 2], [4, 5, 6, 2], [7, 1, 9, 5]],0)==[1, 4, 7]\n","Extracted Code:\n","d\n","\n","Test Case: assert min_length_list([[0], [1, 3], [5, 7], [9, 11], [13, 15, 17]])==(1, [0])\n","Extracted Code:\n","d\n","\n","Test Case: assert check_equilateral(6,8,12)==False \n","Extracted Code:\n","d\n","\n","Test Case: assert parallelogram_area(10,20)==200\n","Extracted Code:\n","d\n","\n","Test Case: assert check_Equality(\"abcda\") == \"Equal\"\n","Extracted Code:\n","d\n","\n","Test Case: assert counting_sort([1,23,4,5,6,7,8]) == [1, 4, 5, 6, 7, 8, 23]\n","Extracted Code:\n","d\n","\n","Test Case: assert tn_gp(1,5,2)==16\n","Extracted Code:\n","d\n","\n","Test Case: assert check(70) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Max_Num([1,2,3],3) == 321\n","Extracted Code:\n","d\n","\n","Test Case: assert opposite_Signs(1,-2) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert is_octagonal(5) == 65\n","Extracted Code:\n","d\n","\n","Test Case: assert max_len_sub([2, 5, 6, 3, 7, 6, 5, 8], 8) == 5\n","Extracted Code:\n","c\n","\n","Test Case: assert count_Substrings('112112',6) == 6\n","Extracted Code:\n","d\n","\n","Test Case: assert smallest_num([10, 20, 1, 45, 99]) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert max_difference([(3, 5), (1, 7), (10, 3), (1, 2)]) == 7\n","Extracted Code:\n","c\n","\n","Test Case: assert subject_marks([('English', 88), ('Science', 90), ('Maths', 97), ('Social sciences', 82)])==[('Social sciences', 82), ('English', 88), ('Science', 90), ('Maths', 97)]\n","Extracted Code:\n","d\n","\n","Test Case: assert recursive_list_sum(([1, 2, [3,4],[5,6]]))==21\n","Extracted Code:\n","d\n","\n","Test Case: assert pos_count([1,-2,3,-4]) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert bell_number(2)==2\n","Extracted Code:\n","N\n","\n","Test Case: assert is_Monotonic([6, 5, 4, 4]) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert is_sublist([2,4,3,5,7],[3,7])==False\n","Extracted Code:\n","d\n","\n","Test Case: assert get_equal([(11, 22, 33), (44, 55, 66)], 3) == 'All tuples have same length'\n","Extracted Code:\n","d\n","\n","Test Case: assert comb_sort([5, 15, 37, 25, 79]) == [5, 15, 25, 37, 79]\n","Extracted Code:\n","d\n","\n","Test Case: assert dif_Square(5) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert multiple_split('Forces of the \\ndarkness*are coming into the play.') == ['Forces of the ', 'darkness', 'are coming into the play.']\n","Extracted Code:\n","d\n","\n","Test Case: assert is_samepatterns([\"red\",\"green\",\"green\"], [\"a\", \"b\", \"b\"])==True \n","Extracted Code:\n","N\n","\n","Test Case: assert find_tuples([(6, 24, 12), (7, 9, 6), (12, 18, 21)], 6) == '[(6, 24, 12)]'\n","Extracted Code:\n","c\n","\n","Test Case: assert count_Squares(4,3) == 20\n","Extracted Code:\n","d\n","\n","Test Case: assert is_Diff (12345) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert count_With_Odd_SetBits(5) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert word_len(\"Hadoop\") == False\n","Extracted Code:\n","N\n","\n","Test Case: assert tetrahedral_number(5) == 35.0\n","Extracted Code:\n","d\n","\n","Test Case: assert zip_tuples((7, 8, 4, 5, 9, 10),(1, 5, 6) ) == [(7, 1), (8, 5), (4, 6), (5, 1), (9, 5), (10, 6)]\n","Extracted Code:\n","d\n","\n","Test Case: assert volume_sphere(10)==4188.790204786391\n","Extracted Code:\n","d\n","\n","Test Case: assert get_Char(\"abc\") == \"f\"\n","Extracted Code:\n","d\n","\n","Test Case: assert sequence(10) == 6\n","Extracted Code:\n","c\n","\n","Test Case: assert surfacearea_sphere(10)==1256.6370614359173\n","Extracted Code:\n","d\n","\n","Test Case: assert centered_hexagonal_number(10) == 271\n","Extracted Code:\n","d\n","\n","Test Case: assert merge_dictionaries_three({ \"R\": \"Red\", \"B\": \"Black\", \"P\": \"Pink\" }, { \"G\": \"Green\", \"W\": \"White\" },{ \"O\": \"Orange\", \"W\": \"White\", \"B\": \"Black\" })=={'B': 'Black', 'R': 'Red', 'P': 'Pink', 'G': 'Green', 'W': 'White', 'O': 'Orange'}\n","Extracted Code:\n","d\n","\n","Test Case: assert freq_count([10,10,10,10,20,20,20,20,40,40,50,50,30])==({10: 4, 20: 4, 40: 2, 50: 2, 30: 1}) \n","Extracted Code:\n","d\n","\n","Test Case: assert closest_num(11) == 10\n","Extracted Code:\n","d\n","\n","Test Case: assert len_log([\"python\",\"PHP\",\"bigdata\"]) == 7\n","Extracted Code:\n","d\n","\n","Test Case: assert find_substring([\"red\", \"black\", \"white\", \"green\", \"orange\"],\"ack\")==True\n","Extracted Code:\n","d\n","\n","Test Case: assert is_undulating(\"1212121\") == True\n","Extracted Code:\n","d\n","\n","Test Case: assert power(3,4) == 81\n","Extracted Code:\n","c\n","\n","Test Case: assert index_minimum([('Rash', 143), ('Manjeet', 200), ('Varsha', 100)]) == 'Varsha'\n","Extracted Code:\n","d\n","\n","Test Case: assert Find_Min_Length([[1],[1,2]]) == 1\n","Extracted Code:\n","c\n","\n","Test Case: assert divisor(15) == 4 \n","Extracted Code:\n","d\n","\n","Test Case: assert frequency_lists([[1, 2, 3, 2], [4, 5, 6, 2], [7, 8, 9, 5]])=={1: 1, 2: 3, 3: 1, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1, 9: 1}\n","Extracted Code:\n","d\n","\n","Test Case: assert multiply_num((8, 2, 3, -1, 7))==-67.2\n","Extracted Code:\n","d\n","\n","Test Case: assert decimal_to_binary(8) == '1000'\n","Extracted Code:\n","d\n","\n","Test Case: assert next_smallest_palindrome(99)==101\n","Extracted Code:\n","c\n","\n","Test Case: assert kth_element([12,3,5,7,19], 5, 2) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert snake_to_camel('python_program')=='PythonProgram'\n","Extracted Code:\n","d\n","\n","Test Case: assert eulerian_num(3, 1) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert sort_sublists(([\"green\", \"orange\"], [\"black\", \"white\"], [\"white\", \"black\", \"orange\"]))==[['green', 'orange'], ['black', 'white'], ['black', 'orange', 'white']]\n","Extracted Code:\n","d\n","\n","Test Case: assert count([True,False,True]) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert add_lists([5, 6, 7], (9, 10)) == (9, 10, 5, 6, 7)\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Hexadecimal(10,15) == 6\n","Extracted Code:\n","d\n","\n","Test Case: assert merge_sorted_list([25, 24, 15, 4, 5, 29, 110],[19, 20, 11, 56, 25, 233, 154],[24, 26, 54, 48])==[4, 5, 11, 15, 19, 20, 24, 24, 25, 25, 26, 29, 48, 54, 56, 110, 154, 233]\n","Extracted Code:\n","N\n","\n","Test Case: assert odd_Equivalent(\"011001\",6) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert extract_missing([(6, 9), (15, 34), (48, 70)], 2, 100) == [(2, 6), (9, 100), (9, 15), (34, 100), (34, 48), (70, 100)]\n","Extracted Code:\n","d\n","\n","Test Case: assert common_in_nested_lists([[12, 18, 23, 25, 45], [7, 12, 18, 24, 28], [1, 5, 8, 12, 15, 16, 18]])==[18, 12]\n","Extracted Code:\n","d\n","\n","Test Case: assert perimeter(2,4) == 12\n","Extracted Code:\n","d\n","\n","Test Case: assert check_integer(\"python\")==False\n","Extracted Code:\n","d\n","\n","Test Case: assert assign_freq([(6, 5, 8), (2, 7), (6, 5, 8), (6, 5, 8), (9, ), (2, 7)] ) == '[(6, 5, 8, 3), (2, 7, 2), (9, 1)]'\n","Extracted Code:\n","c\n","\n","Test Case: assert empty_dit([{},{},{}])==True\n","Extracted Code:\n","d\n","\n","Test Case: assert tuple_to_int((1,2,3))==123\n","Extracted Code:\n","d\n","\n","Test Case: assert list_to_float( [(\"3\", \"4\"), (\"1\", \"26.45\"), (\"7.32\", \"8\"), (\"4\", \"8\")] ) == '[(3.0, 4.0), (1.0, 26.45), (7.32, 8.0), (4.0, 8.0)]'\n","Extracted Code:\n","d\n","\n","Test Case: assert string_to_list(\"python programming\")==['python','programming']\n","Extracted Code:\n","d\n","\n","Test Case: assert search([1,1,2,2,3],5) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert max_product_tuple([(2, 7), (2, 6), (1, 8), (4, 9)] )==36\n","Extracted Code:\n","d\n","\n","Test Case: assert check_triplet([2, 7, 4, 0, 9, 5, 1, 3], 8, 6, 0) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert smartNumber(1) == 30\n","Extracted Code:\n","N\n","\n","Test Case: assert amicable_numbers_sum(999)==504\n","Extracted Code:\n","N\n","\n","Test Case: assert angle_complex(0,1j)==1.5707963267948966 \n","Extracted Code:\n","d\n","\n","Test Case: assert find_length(\"11000010001\", 11) == 6\n","Extracted Code:\n","c\n","\n","Test Case: assert sum(10,15) == 6\n","Extracted Code:\n","d\n","\n","Test Case: assert multiply_int(10,20)==200\n","Extracted Code:\n","d\n","\n","Test Case: assert long_words(3,\"python is a programming language\")==['python','programming','language']\n","Extracted Code:\n","d\n","\n","Test Case: assert magic_square_test([[7, 12, 1, 14], [2, 13, 8, 11], [16, 3, 10, 5], [9, 6, 15, 4]])==True\n","Extracted Code:\n","d\n","\n","Test Case: assert max_occurrences([2,3,8,4,7,9,8,2,6,5,1,6,1,2,3,2,4,6,9,1,2])==(2, 5)\n","Extracted Code:\n","d\n","\n","Test Case: assert reverse_vowels(\"Python\") == \"Python\"\n","Extracted Code:\n","d\n","\n","Test Case: assert tup_string(('e', 'x', 'e', 'r', 'c', 'i', 's', 'e', 's'))==(\"exercises\")\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_negativenum([2, 4, -6, -9, 11, -12, 14, -5, 17])==-32\n","Extracted Code:\n","d\n","\n","Test Case: assert check_last([5,7,10],3,1) == \"ODD\"\n","Extracted Code:\n","N\n","\n","Test Case: assert hexagonal_num(10) == 190\n","Extracted Code:\n","d\n","\n","Test Case: assert cal_electbill(75)==246.25\n","Extracted Code:\n","d\n","\n","Test Case: assert zero_count([0, 1, 2, -1, -5, 6, 0, -3, -2, 3, 4, 6, 8])==0.15\n","Extracted Code:\n","d\n","\n","Test Case: assert is_Sum_Of_Powers_Of_Two(10) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert circle_circumference(10)==62.830000000000005\n","Extracted Code:\n","d\n","\n","Test Case: assert extract_singly([(3, 4, 5), (4, 5, 7), (1, 4)]) == [3, 4, 5, 7, 1]\n","Extracted Code:\n","d\n","\n","Test Case: assert pancake_sort([15, 79, 25, 38, 69]) == [15, 25, 38, 69, 79]\n","Extracted Code:\n","d\n","\n","Test Case: assert count_samepair([1,2,3,4,5,6,7,8],[2,2,3,1,2,6,7,9],[2,1,3,1,2,6,7,9])==3\n","Extracted Code:\n","d\n","\n","Test Case: assert find_lists(([1, 2, 3, 4], [5, 6, 7, 8])) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_Pairs([1,8,9,15,16],5) == 74\n","Extracted Code:\n","d\n","\n","Test Case: assert max_Abs_Diff((2,1,5,3),4) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert ascii_value_string(\"python\")==112\n","Extracted Code:\n","d\n","\n","Test Case: assert max_path_sum([[1, 0, 0], [4, 8, 0], [1, 5, 3]], 2, 2) == 14\n","Extracted Code:\n","N\n","\n","Test Case: assert sum_digits_twoparts(35)==17\n","Extracted Code:\n","d\n","\n","Test Case: assert longest_subseq_with_diff_one([1, 2, 3, 4, 5, 3, 2], 7) == 6\n","Extracted Code:\n","c\n","\n","Test Case: assert does_Contain_B(1,7,3) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert is_coprime(17,13) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert merge_sort([3, 4, 2, 6, 5, 7, 1, 9]) == [1, 2, 3, 4, 5, 6, 7, 9]\n","Extracted Code:\n","d\n","\n","Test Case: assert parabola_vertex(5,3,2)==(-0.3, 1.55)\n","Extracted Code:\n","d\n","\n","Test Case: assert specified_element([[1, 2, 3, 2], [4, 5, 6, 2], [7, 1, 9, 5]],0)==[1, 4, 7]\n","Extracted Code:\n","N\n","\n","Test Case: assert even_bit_toggle_number(10) == 0\n","Extracted Code:\n","d\n","\n","Test Case: assert tuple_int_str((('333', '33'), ('1416', '55')))==((333, 33), (1416, 55))\n","Extracted Code:\n","d\n","\n","Test Case: assert encode_list([1,1,2,3,4,4.3,5,1])==[[2, 1], [1, 2], [1, 3], [1, 4], [1, 4.3], [1, 5], [1, 1]]\n","Extracted Code:\n","d\n","\n","Test Case: assert min_Ops([2,2,2,2],4,3) == 0\n","Extracted Code:\n","d\n","\n","Test Case: assert month_season('January',4)==('winter')\n","Extracted Code:\n","d\n","\n","Test Case: assert solution(2, 3, 7) == ('x = ', 2, ', y = ', 1)\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_elements([1,2,3,4,5,6,7,8,9,10],[2,4,6,8])==[1, 3, 5, 7, 9, 10]\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_series(6)==12\n","Extracted Code:\n","d\n","\n","Test Case: assert area_polygon(4,20)==400.00000000000006\n","Extracted Code:\n","d\n","\n","Test Case: assert areEquivalent(36,57) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert count_char_position(\"xbcefg\") == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert find_even_Pair([5,4,7,2,1],5) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert next_Power_Of_2(0) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert frequency([1,2,3],4) == 0\n","Extracted Code:\n","d\n","\n","Test Case: assert get_pell(4) == 12\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_range_list( [2,1,5,6,8,3,4,9,10,11,8,12],8,10)==29\n","Extracted Code:\n","d\n","\n","Test Case: assert perimeter_pentagon(5)==25\n","Extracted Code:\n","d\n","\n","Test Case: assert count_occurance(\"letstdlenstdporstd\") == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_splchar('python  @#&^%$*program123')==('pythonprogram123')\n","Extracted Code:\n","d\n","\n","Test Case: assert group_keyvalue([('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)])=={'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}\n","Extracted Code:\n","d\n","\n","Test Case: assert is_valid_parenthese(\"(){}[]\")==True\n","Extracted Code:\n","d\n","\n","Test Case: assert perimeter_triangle(10,20,30)==60\n","Extracted Code:\n","d\n","\n","Test Case: assert answer(3,8) == (3,6)\n","Extracted Code:\n","d\n","\n","Test Case: assert string_literals(['language'],'python language')==('Matched!')\n","Extracted Code:\n","d\n","\n","Test Case: assert is_num_keith(14) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert distance_lat_long(23.5,67.5,25.5,69.5)==12179.372041317429\n","Extracted Code:\n","d\n","\n","Test Case: assert common_prefix([\"tablets\", \"tables\", \"taxi\", \"tamarind\"], 4) == 'ta'\n","Extracted Code:\n","d\n","\n","Test Case: assert find_character(\"ThisIsGeeksforGeeks\") == (['T', 'I', 'G', 'G'], ['h', 'i', 's', 's', 'e', 'e', 'k', 's', 'f', 'o', 'r', 'e', 'e', 'k', 's'], [], [])\n","Extracted Code:\n","d\n","\n","Test Case: assert count_pairs([1, 5, 3, 4, 2], 5, 3) == 2\n","Extracted Code:\n","c\n","\n","Test Case: assert greater_specificnum([220, 330, 500],200)==True\n","Extracted Code:\n","d\n","\n","Test Case: assert parabola_focus(5,3,2)==(-0.3, 1.6)\n","Extracted Code:\n","d\n","\n","Test Case: assert check_literals('The quick brown fox jumps over the lazy dog.',['fox']) == 'Matched!'\n","Extracted Code:\n","d\n","\n","Test Case: assert longest_common_subsequence(\"AGGTAB\" , \"GXTXAYB\", 6, 7) == 4\n","Extracted Code:\n","N\n","\n","Test Case: assert prod_Square(25) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert first_Missing_Positive([1,2,3,-1,5],5) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Intgral_Points(1,1,4,4) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert check_monthnumber(\"February\")==False\n","Extracted Code:\n","d\n","\n","Test Case: assert check_String('thishasboth29') == True\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_tuple((1, 3, 5, 2, 3, 5, 1, 1, 3)) == (1, 2, 3, 5)\n","Extracted Code:\n","d\n","\n","Test Case: assert octal_To_Decimal(25) == 21\n","Extracted Code:\n","d\n","\n","Test Case: assert first([1,2,3,4,5,6,6],6,6) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_tuples([(4, 5), (4, ), (8, 6, 7), (1, ), (3, 4, 6, 7)] , 1) == [(4, 5), (8, 6, 7), (3, 4, 6, 7)]\n","Extracted Code:\n","d\n","\n","Test Case: assert find_exponentio((10, 4, 5, 6), (5, 6, 7, 5)) == (100000, 4096, 78125, 7776)\n","Extracted Code:\n","d\n","\n","Test Case: assert largest_triangle(4,2)==10.392304845413264\n","Extracted Code:\n","d\n","\n","Test Case: assert highest_Power_of_2(10) == 8\n","Extracted Code:\n","d\n","\n","Test Case: assert position_max([12,33,23,10,67,89,45,667,23,12,11,10,54])==[7]\n","Extracted Code:\n","d\n","\n","Test Case: assert chkList(['one','one','one']) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_even(\"python\")==(\"pto\")\n","Extracted Code:\n","d\n","\n","Test Case: assert hamming_Distance(4,8) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert count(\"abcc\",\"c\") == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert inversion_elements((7, 8, 9, 1, 10, 7)) == (-8, -9, -10, -2, -11, -8)\n","Extracted Code:\n","N\n","\n","Test Case: assert concatenate_elements((\"DSP \", \"IS \", \"BEST \", \"FOR \", \"ALL \", \"UTS\")) == ('DSP IS ', 'IS BEST ', 'BEST FOR ', 'FOR ALL ', 'ALL UTS')\n","Extracted Code:\n","d\n","\n","Test Case: assert find_longest_repeating_subseq(\"AABEBCDD\") == 3\n","Extracted Code:\n","c\n","\n","Test Case: assert is_decimal('123.11') == True\n","Extracted Code:\n","d\n","\n","Test Case: assert heap_replace( [25, 44, 68, 21, 39, 23, 89],21)==[21, 25, 23, 44, 39, 68, 89]\n","Extracted Code:\n","d\n","\n","Test Case: assert is_allowed_specific_char(\"ABCDEFabcdef123450\") == True\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Num(2) == 1\n","Extracted Code:\n","c\n","\n","Test Case: assert fourth_Power_Sum(2) == 17\n","Extracted Code:\n","d\n","\n","Test Case: assert concatenate_strings((\"Manjeet\", \"Nikhil\", \"Akshat\"), (\" Singh\", \" Meherwal\", \" Garg\")) == ('Manjeet Singh', 'Nikhil Meherwal', 'Akshat Garg')\n","Extracted Code:\n","d\n","\n","Test Case: assert degree_radian(90)==5156.620156177409\n","Extracted Code:\n","d\n","\n","Test Case: assert decode_list([[2, 1], 2, 3, [2, 4], 5,1])==[1,1,2,3,4,4,5,1]\n","Extracted Code:\n","d\n","\n","Test Case: assert check_subset_list([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],[[12, 18, 23, 25, 45], [7, 11, 19, 24, 28], [1, 5, 8, 18, 15, 16]])==False\n","Extracted Code:\n","N\n","\n","Test Case: assert first_Repeated_Char(\"Google\") == \"o\"\n","Extracted Code:\n","d\n","\n","Test Case: assert min_Operations(2,4) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert extract_min_max((5, 20, 3, 7, 6, 8), 2) == (3, 5, 8, 20)\n","Extracted Code:\n","d\n","\n","Test Case: assert replace_max_specialchar('Python language, Programming language.',2)==('Python:language: Programming language.')\n","Extracted Code:\n","c\n","\n","Test Case: assert first_even ([1, 3, 5, 7, 4, 1, 6, 8]) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert check_type((5, 6, 7, 3, 5, 6) ) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert is_majority([1, 2, 3, 3, 3, 3, 10], 7, 3) == True\n","Extracted Code:\n","c\n","\n","Test Case: assert count_Set_Bits(2) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Min([1,2,3,4,5],0,4) == 1\n","Extracted Code:\n","N\n","\n","Test Case: assert odd_values_string('abcdef') == 'ace'\n","Extracted Code:\n","d\n","\n","Test Case: assert min_of_three(10,20,0)==0\n","Extracted Code:\n","d\n","\n","Test Case: assert all_Bits_Set_In_The_Given_Range(4,1,2) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert re_arrange_array([-1, 2, -3, 4, 5, 6, -7, 8, 9], 9) == [-1, -3, -7, 4, 5, 6, 2, 8, 9]\n","Extracted Code:\n","d\n","\n","Test Case: assert replace_blank(\"hello people\",'@')==(\"hello@people\")\n","Extracted Code:\n","N\n","\n","Test Case: assert max_sum([[1], [2,1], [3,3,2]], 3) == 6\n","Extracted Code:\n","d\n","\n","Test Case: assert larg_nnum([10, 20, 50, 70, 90, 20, 50, 40, 60, 80, 100],2)==[100,90]\n","Extracted Code:\n","d\n","\n","Test Case: assert lateralsuface_cylinder(10,5)==314.15000000000003\n","Extracted Code:\n","d\n","\n","Test Case: assert volume_cube(3)==27\n","Extracted Code:\n","d\n","\n","Test Case: assert even_bit_set_number(10) == 10\n","Extracted Code:\n","d\n","\n","Test Case: assert No_of_Triangle(4,2) == 7\n","Extracted Code:\n","d\n","\n","Test Case: assert check_occurences([(3, 1), (1, 3), (2, 5), (5, 2), (6, 3)] ) == {(1, 3): 2, (2, 5): 2, (3, 6): 1}\n","Extracted Code:\n","d\n","\n","Test Case: assert number_of_substrings(\"abc\") == 6\n","Extracted Code:\n","d\n","\n","Test Case: assert get_total_number_of_sequences(10, 4) == 4\n","Extracted Code:\n","c\n","\n","Test Case: assert replace_list([1, 3, 5, 7, 9, 10],[2, 4, 6, 8])==[1, 3, 5, 7, 9, 2, 4, 6, 8]\n","Extracted Code:\n","d\n","\n","Test Case: assert array_3d(6,4,3)==[[['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*']], [['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*']], [['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*'], ['*', '*', '*', '*', '*', '*']]]\n","Extracted Code:\n","d\n","\n","Test Case: assert count_charac(\"python programming\")==18\n","Extracted Code:\n","d\n","\n","Test Case: assert sort_on_occurence([(1, 'Jake'), (2, 'Bob'), (1, 'Cara')]) == [(1, 'Jake', 'Cara', 2), (2, 'Bob', 1)]\n","Extracted Code:\n","c\n","\n","Test Case: assert next_Perfect_Square(35) == 36\n","Extracted Code:\n","d\n","\n","Test Case: assert max_sum([1, 15, 51, 45, 33, 100, 12, 18, 9], 9) == 194\n","Extracted Code:\n","c\n","\n","Test Case: assert babylonian_squareroot(10)==3.162277660168379\n","Extracted Code:\n","N\n","\n","Test Case: assert lps(\"TENS FOR TENS\") == 5 \n","Extracted Code:\n","d\n","\n","Test Case: assert harmonic_sum(7) == 2.5928571428571425\n","Extracted Code:\n","d\n","\n","Test Case: assert intersection_array([1, 2, 3, 5, 7, 8, 9, 10],[1, 2, 4, 8, 9])==[1, 2, 8, 9]\n","Extracted Code:\n","c\n","\n","Test Case: assert count_X((10, 8, 5, 2, 10, 15, 10, 8, 5, 8, 8, 2),4) == 0\n","Extracted Code:\n","d\n","\n","Test Case: assert insert_element(['Red', 'Green', 'Black'] ,'c')==['c', 'Red', 'c', 'Green', 'c', 'Black'] \n","Extracted Code:\n","N\n","\n","Test Case: assert convert(1) == (1.0, 0.0)\n","Extracted Code:\n","N\n","\n","Test Case: assert count_integer([1,2,'abc',1.2]) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert words_ae(\"python programe\")==['ame']\n","Extracted Code:\n","d\n","\n","Test Case: assert combinations_colors( [\"Red\",\"Green\",\"Blue\"],1)==[('Red',), ('Green',), ('Blue',)]\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Primes_nums(5) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert swap_numbers(10,20)==(20,10)\n","Extracted Code:\n","d\n","\n","Test Case: assert count_odd([1, 2, 3, 5, 7, 8, 10])==4\n","Extracted Code:\n","d\n","\n","Test Case: assert maximize_elements(((1, 3), (4, 5), (2, 9), (1, 10)), ((6, 7), (3, 9), (1, 1), (7, 3))) == ((6, 7), (4, 9), (2, 9), (7, 10))\n","Extracted Code:\n","d\n","\n","Test Case: assert newman_prime(3) == 7 \n","Extracted Code:\n","d\n","\n","Test Case: assert division_elements((10, 4, 6, 9),(5, 2, 3, 3)) == (2, 2, 2, 3)\n","Extracted Code:\n","d\n","\n","Test Case: assert split_two_parts([1,1,2,3,4,4,5,1],3)==([1, 1, 2], [3, 4, 4, 5, 1])\n","Extracted Code:\n","d\n","\n","Test Case: assert merge_dict({'a': 100, 'b': 200},{'x': 300, 'y': 200})=={'x': 300, 'y': 200, 'a': 100, 'b': 200}\n","Extracted Code:\n","d\n","\n","Test Case: assert dog_age(12)==61\n","Extracted Code:\n","d\n","\n","Test Case: assert list_split(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n'],3)==[['a', 'd', 'g', 'j', 'm'], ['b', 'e', 'h', 'k', 'n'], ['c', 'f', 'i', 'l']] \n","Extracted Code:\n","d\n","\n","Test Case: assert lateralsurface_cube(5)==100\n","Extracted Code:\n","N\n","\n","Test Case: assert square_Sum(2) == 10\n","Extracted Code:\n","d\n","\n","Test Case: assert find_star_num(3) == 37\n","Extracted Code:\n","d\n","\n","Test Case: assert ascii_value('A')==65\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_even_and_even_index([5, 6, 12, 1, 18, 8],6) == 30\n","Extracted Code:\n","d\n","\n","Test Case: assert even_Power_Sum(2) == 1056\n","Extracted Code:\n","d\n","\n","Test Case: assert rear_extract([(1, 'Rash', 21), (2, 'Varsha', 20), (3, 'Kil', 19)]) == [21, 20, 19]\n","Extracted Code:\n","d\n","\n","Test Case: assert substract_elements((10, 4, 5), (2, 5, 18)) == (8, -1, -13)\n","Extracted Code:\n","d\n","\n","Test Case: assert even_binomial_Coeff_Sum(4) == 8\n","Extracted Code:\n","d\n","\n","Test Case: assert get_Position([2,5,4],3,2) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert volume_cylinder(10,5)==1570.7500000000002\n","Extracted Code:\n","d\n","\n","Test Case: assert dict_filter({'Cierra Vega': 175, 'Alden Cantrell': 180, 'Kierra Gentry': 165, 'Pierre Cox': 190},170)=={'Cierra Vega': 175, 'Alden Cantrell': 180, 'Pierre Cox': 190}\n","Extracted Code:\n","d\n","\n","Test Case: assert count_first_elements((1, 5, 7, (4, 6), 10) ) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert is_num_decagonal(3) == 27\n","Extracted Code:\n","d\n","\n","Test Case: assert sequential_search([11,23,58,31,56,77,43,12,65,19],31) == (True, 3)\n","Extracted Code:\n","d\n","\n","Test Case: assert all_unique([1,2,3]) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert sub_list([1, 2, 3],[4,5,6])==[-3,-3,-3]\n","Extracted Code:\n","d\n","\n","Test Case: assert validate(1234) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert check_element([\"green\", \"orange\", \"black\", \"white\"],'blue')==False\n","Extracted Code:\n","d\n","\n","Test Case: assert text_match_two_three(\"ac\")==('Not matched!')\n","Extracted Code:\n","d\n","\n","Test Case: assert max_sub_array_sum_repeated([10, 20, -30, -1], 4, 3) == 30\n","Extracted Code:\n","c\n","\n","Test Case: assert square_Sum(2) == 20\n","Extracted Code:\n","d\n","\n","Test Case: assert modular_inverse([ 1, 6, 4, 5 ], 4, 7) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert odd_Days(100) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert max_length([[0], [1, 3], [5, 7], [9, 11], [13, 15, 17]])==(3, [13, 15, 17])\n","Extracted Code:\n","N\n","\n","Test Case: assert count_no_of_ways(2, 4) == 16\n","Extracted Code:\n","c\n","\n","Test Case: assert find(10,3) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert otherside_rightangle(7,8)==10.63014581273465\n","Extracted Code:\n","d\n","\n","Test Case: assert max_val(['Python', 3, 2, 4, 5, 'version'])==5\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_div(8)==7\n","Extracted Code:\n","d\n","\n","Test Case: assert get_Inv_Count([1,20,6,4,5],5) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert flatten_list([0, 10, [20, 30], 40, 50, [60, 70, 80], [90, 100, 110, 120]])==[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120]\n","Extracted Code:\n","d\n","\n","Test Case: assert intersection_nested_lists( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],[[12, 18, 23, 25, 45], [7, 11, 19, 24, 28], [1, 5, 8, 18, 15, 16]])==[[12], [7, 11], [1, 5, 8]]\n","Extracted Code:\n","d\n","\n","Test Case: assert max_aggregate([('Juan Whelan',90),('Sabah Colley',88),('Peter Nichols',7),('Juan Whelan',122),('Sabah Colley',84)])==('Juan Whelan', 212)\n","Extracted Code:\n","d\n","\n","Test Case: assert count_binary_seq(1) == 2.0\n","Extracted Code:\n","c\n","\n","Test Case: assert dict_depth({'a':1, 'b': {'c': {'d': {}}}})==4\n","Extracted Code:\n","d\n","\n","Test Case: assert set_Bit_Number(6) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert solve([1,0,2],3) == True\n","Extracted Code:\n","c\n","\n","Test Case: assert find_Element([1,2,3,4,5],[[0,2],[0,3]],2,1) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert start_withp([\"Python PHP\", \"Java JavaScript\", \"c c++\"])==('Python', 'PHP')\n","Extracted Code:\n","d\n","\n","Test Case: assert max_sum_increasing_subseq([1, 101, 2, 3, 100, 4, 5 ], 7, 4, 6) == 11\n","Extracted Code:\n","c\n","\n","Test Case: assert colon_tuplex((\"HELLO\", 5, [], True) ,2,50)==(\"HELLO\", 5, [50], True) \n","Extracted Code:\n","d\n","\n","Test Case: assert large_product([1, 2, 3, 4, 5, 6],[3, 6, 8, 9, 10, 6],3)==[60, 54, 50]\n","Extracted Code:\n","d\n","\n","Test Case: assert maximum(5,10) == 10\n","Extracted Code:\n","d\n","\n","Test Case: assert string_to_tuple(\"python 3.0\")==('p', 'y', 't', 'h', 'o', 'n', '3', '.', '0')\n","Extracted Code:\n","d\n","\n","Test Case: assert set_left_most_unset_bit(10) == 14\n","Extracted Code:\n","N\n","\n","Test Case: assert volume_cone(5,12)==314.15926535897927\n","Extracted Code:\n","d\n","\n","Test Case: assert pos_nos([-1,-2,1,2]) == 1,2\n","Extracted Code:\n","d\n","\n","Test Case: assert max_sum_rectangular_grid([ [1, 4, 5], [2, 0, 0 ] ], 3) == 7\n","Extracted Code:\n","c\n","\n","Test Case: assert find_Max_Len_Even(\"python language\") == \"language\"\n","Extracted Code:\n","d\n","\n","Test Case: assert find_last_occurrence([2, 5, 5, 5, 6, 6, 8, 9, 9, 9], 5) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert modified_encode([1,1,2,3,4,4,5,1])==[[2, 1], 2, 3, [2, 4], 5, 1]\n","Extracted Code:\n","d\n","\n","Test Case: assert max_volume(8) == 18\n","Extracted Code:\n","d\n","\n","Test Case: assert find_long_word('Please move back to strem') == ['strem']\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_difference(12)==5434\n","Extracted Code:\n","d\n","\n","Test Case: assert find_demlo(\"111111\") == '12345654321'\n","Extracted Code:\n","d\n","\n","Test Case: assert position_min([12,33,23,10,67,89,45,667,23,12,11,10,54])==[3,11]\n","Extracted Code:\n","d\n","\n","Test Case: assert re_arrange([-5, -2, 5, 2, 4,\t7, 1, 8, 0, -8], 10) == [-5, 5, -2, 2, -8, 4, 7, 1, 8, 0]\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_of_alternates((5, 6, 3, 6, 10, 34)) == (46, 18)\n","Extracted Code:\n","d\n","\n","Test Case: assert get_Min_Squares(6) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert most_occurrences([\"UTS is best for RTF\", \"RTF love UTS\", \"UTS is best\"] ) == 'UTS'\n","Extracted Code:\n","d\n","\n","Test Case: assert check_isosceles(6,8,12)==False \n","Extracted Code:\n","d\n","\n","Test Case: assert rotate_left([1, 2, 3, 4, 5, 6, 7, 8, 9, 10],3,4)==[4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4]\n","Extracted Code:\n","d\n","\n","Test Case: assert neg_count([-1,-2,3,-4,-5]) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert find_char('For the four consumer complaints contact manager AKR reddy') == ['For', 'the', 'four', 'AKR', 'reddy']\n","Extracted Code:\n","d\n","\n","Test Case: assert count_unset_bits(2) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert char_frequency('python')=={'p': 1, 'y': 1, 't': 1, 'h': 1, 'o': 1, 'n': 1}\n","Extracted Code:\n","d\n","\n","Test Case: assert Sort([['a', 10], ['b', 5], ['c', 20], ['d', 15]]) == [['b', 5], ['a', 10], ['d', 15], ['c', 20]]\n","Extracted Code:\n","d\n","\n","Test Case: assert check_Validity(1,2,3) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert ap_sum(1,5,2)==25\n","Extracted Code:\n","d\n","\n","Test Case: assert check_monthnum(\"February\")==True\n","Extracted Code:\n","d\n","\n","Test Case: assert text_match_word(\"python.\")==('Found a match!')\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Substring_With_Equal_Ends('aba') == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Divisor(2,2) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_three_smallest_nums([10,20,30,40,50,60,7]) == 37\n","Extracted Code:\n","d\n","\n","Test Case: assert set_to_tuple({1, 2, 3, 4, 5}) == (1, 2, 3, 4, 5)\n","Extracted Code:\n","d\n","\n","Test Case: assert find_minimum_range([[3, 6, 8, 10, 15], [1, 5, 12], [4, 8, 15, 16], [2, 6]]) == (4, 6)\n","Extracted Code:\n","c\n","\n","Test Case: assert dig_let(\"python\")==(6,0)\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Odd_Squares(5,100) == 8\n","Extracted Code:\n","d\n","\n","Test Case: assert diff_consecutivenums([1, 1, 3, 4, 4, 5, 6, 7])==[0, 2, 1, 0, 1, 1, 1]\n","Extracted Code:\n","d\n","\n","Test Case: assert zigzag(4, 3) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Squares(4,3) == 20\n","Extracted Code:\n","d\n","\n","Test Case: assert find_ways(4) == 2\n","Extracted Code:\n","N\n","\n","Test Case: assert check(\"01010101010\") == \"Yes\"\n","Extracted Code:\n","d\n","\n","Test Case: assert minimum_Length(\"mnm\") == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert first_Element([0,1,2,3,4,5],6,1) == 0\n","Extracted Code:\n","d\n","\n","Test Case: assert unique_Characters('aba') == False\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_column([[1, 2, 3], [2, 4, 5], [1, 1, 1]],0)==[[2, 3], [4, 5], [1, 1]]\n","Extracted Code:\n","d\n","\n","Test Case: assert tn_ap(1,5,2)==9\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Rectangles(2) == 8\n","Extracted Code:\n","d\n","\n","Test Case: assert find_angle(47,89)==44\n","Extracted Code:\n","d\n","\n","Test Case: assert find_max([(2, 4), (6, 7), (5, 1), (6, 10), (8, 7)]) == 10\n","Extracted Code:\n","d\n","\n","Test Case: assert moddiv_list([4,5,6],[1, 2, 3])==[0, 1, 0]\n","Extracted Code:\n","d\n","\n","Test Case: assert Check_Solution(1,3,2) == \"Yes\"\n","Extracted Code:\n","d\n","\n","Test Case: assert get_carol(2) == 7\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_empty([[], [], [], 'Red', 'Green', [1,2], 'Blue', [], []])==['Red', 'Green', [1, 2], 'Blue']\n","Extracted Code:\n","d\n","\n","Test Case: assert max_occurrences([1,2,3,1,2,3,12,4,2]) ==  2\n","Extracted Code:\n","d\n","\n","Test Case: assert add_K_element([(1, 3, 4), (2, 4, 6), (3, 8, 1)], 4) == [(5, 7, 8), (6, 8, 10), (7, 12, 5)]\n","Extracted Code:\n","d\n","\n","Test Case: assert min_flip_to_make_string_alternate(\"0001010111\") == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Digit(12345) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert adjacent_num_product([1,2,3,4,5,6]) == 30\n","Extracted Code:\n","d\n","\n","Test Case: assert is_tree_balanced(root) == False\n","Extracted Code:\n","c\n","\n","Test Case: assert repeat_tuples((1, 3), 4) == ((1, 3), (1, 3), (1, 3), (1, 3))\n","Extracted Code:\n","d\n","\n","Test Case: assert lateralsurface_cuboid(8,5,6)==156\n","Extracted Code:\n","N\n","\n","Test Case: assert float_sort([('item1', '12.20'), ('item2', '15.10'), ('item3', '24.5')])==[('item3', '24.5'), ('item2', '15.10'), ('item1', '12.20')] \n","Extracted Code:\n","N\n","\n","Test Case: assert smallest_missing([0, 1, 2, 3, 4, 5, 6], 0, 6) == 7\n","Extracted Code:\n","d\n","\n","Test Case: assert heap_assending([18, 14, 10, 9, 8, 7, 9, 3, 2, 4, 1])==[1, 2, 3, 4, 7, 8, 9, 9, 10, 14, 18]\n","Extracted Code:\n","d\n","\n","Test Case: assert volume_cuboid(1,2,3)==6\n","Extracted Code:\n","d\n","\n","Test Case: assert permute_string('ab')==['ab', 'ba']\n","Extracted Code:\n","c\n","\n","Test Case: assert round_num(4722,10)==4720\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_replica((1, 1, 4, 4, 4, 5, 5, 6, 7, 7)) == (1, 'MSP', 4, 'MSP', 'MSP', 5, 'MSP', 6, 7, 'MSP')\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_Char(\"aba\",'a') == \"b\"\n","Extracted Code:\n","d\n","\n","Test Case: assert move_first([1,2,3,4]) == [4,1,2,3]\n","Extracted Code:\n","d\n","\n","Test Case: assert surfacearea_cuboid(1,2,3)==22\n","Extracted Code:\n","d\n","\n","Test Case: assert multi_list(3,4)==[[0, 0, 0, 0], [0, 1, 2, 3], [0, 2, 4, 6]] \n","Extracted Code:\n","d\n","\n","Test Case: assert index_on_inner_list([('Greyson Fulton', 98, 99), ('Brady Kent', 97, 96), ('Wyatt Knott', 91, 94), ('Beau Turnbull', 94, 98)] ,0)==[('Beau Turnbull', 94, 98), ('Brady Kent', 97, 96), ('Greyson Fulton', 98, 99), ('Wyatt Knott', 91, 94)]\n","Extracted Code:\n","d\n","\n","Test Case: assert find_rotation_count([8, 9, 10, 1, 2, 3, 4, 5, 6, 7]) == 3\n","Extracted Code:\n","c\n","\n","Test Case: assert even_bit_toggle_number(10) == 15\n","Extracted Code:\n","d\n","\n","Test Case: assert frequency_Of_Smallest(5,[1,2,3,4,3]) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert get_perrin(9) == 12\n","Extracted Code:\n","d\n","\n","Test Case: assert swap_count(\"[]][][\") == 2\n","Extracted Code:\n","c\n","\n","Test Case: assert even_or_odd(\"AB3454D\") ==\"Odd\"\n","Extracted Code:\n","d\n","\n","Test Case: assert highest_Power_of_2(10) == 8\n","Extracted Code:\n","d\n","\n","Test Case: assert find_lucas(9) == 76\n","Extracted Code:\n","N\n","\n","Test Case: assert add_string([1,2,3,4],'temp{0}')==['temp1', 'temp2', 'temp3', 'temp4']\n","Extracted Code:\n","d\n","\n","Test Case: assert convert_list_dictionary([\"S001\", \"S002\", \"S003\", \"S004\"],[\"Adina Park\", \"Leyton Marsh\", \"Duncan Boyle\", \"Saim Richards\"] ,[85, 98, 89, 92])==[{'S001': {'Adina Park': 85}}, {'S002': {'Leyton Marsh': 98}}, {'S003': {'Duncan Boyle': 89}}, {'S004': {'Saim Richards': 92}}]\n","Extracted Code:\n","N\n","\n","Test Case: assert get_max_sum(60) == 106\n","Extracted Code:\n","N\n","\n","Test Case: assert max_length_list([[0], [1, 3], [5, 7], [9, 11], [13, 15, 17]])==(3, [13, 15, 17])\n","Extracted Code:\n","N\n","\n","Test Case: assert check_distinct((1, 4, 5, 6, 1, 4)) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert first_non_repeating_character(\"abcabc\") == None\n","Extracted Code:\n","d\n","\n","Test Case: assert check_char(\"abba\") == \"Valid\"\n","Extracted Code:\n","d\n","\n","Test Case: assert median_numbers(25,55,65)==55.0\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_of_digits([10,2,56])==14\n","Extracted Code:\n","d\n","\n","Test Case: assert bitwise_xor((10, 4, 6, 9), (5, 2, 3, 3)) == (15, 6, 5, 10)\n","Extracted Code:\n","d\n","\n","Test Case: assert extract_freq([(3, 4), (1, 2), (4, 3), (5, 6)] ) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert add_nested_tuples(((1, 3), (4, 5), (2, 9), (1, 10)), ((6, 7), (3, 9), (1, 1), (7, 3))) == ((7, 10), (7, 14), (3, 10), (8, 13))\n","Extracted Code:\n","d\n","\n","Test Case: assert ncr_modp(10,2,13)==6\n","Extracted Code:\n","d\n","\n","Test Case: assert is_valid_URL(\"https://www.google.com\") == True\n","Extracted Code:\n","d\n","\n","Test Case: assert minimum(1,2) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert check_tuplex((\"w\", 3, \"r\", \"e\", \"s\", \"o\", \"u\", \"r\", \"c\", \"e\"),'r')==True\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Parity(12) == \"Even Parity\"\n","Extracted Code:\n","d\n","\n","Test Case: assert rearrange_bigger(12)==21\n","Extracted Code:\n","d\n","\n","Test Case: assert k_smallest_pairs([1,3,7],[2,4,6],2)==[[1, 2], [1, 4]]\n","Extracted Code:\n","d\n","\n","Test Case: assert min_product_tuple([(2, 7), (2, 6), (1, 8), (4, 9)] )==8\n","Extracted Code:\n","N\n","\n","Test Case: assert min_val(['Python', 3, 2, 4, 5, 'version'])==2\n","Extracted Code:\n","d\n","\n","Test Case: assert snake_to_camel('android_tv') == 'AndroidTv'\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_odd([1,2,3]) == [2]\n","Extracted Code:\n","d\n","\n","Test Case: assert extract_nth_element([('Greyson Fulton', 98, 99), ('Brady Kent', 97, 96), ('Wyatt Knott', 91, 94), ('Beau Turnbull', 94, 98)] ,0)==['Greyson Fulton', 'Brady Kent', 'Wyatt Knott', 'Beau Turnbull']\n","Extracted Code:\n","d\n","\n","Test Case: assert overlapping([1,2,3,4,5],[6,7,8,9]) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert max_Product([1,2,3,4,7,0,8,4]) == (7,8)\n","Extracted Code:\n","d\n","\n","Test Case: assert breakSum(12) == 13\n","Extracted Code:\n","d\n","\n","Test Case: assert group_tuples([('x', 'y'), ('x', 'z'), ('w', 't')]) == [('x', 'y', 'z'), ('w', 't')]\n","Extracted Code:\n","d\n","\n","Test Case: assert Find_Max([['A'],['A','B'],['A','B','C']]) == ['A','B','C']\n","Extracted Code:\n","d\n","\n","Test Case: assert round_and_sum([22.4, 4.0, -16.22, -9.10, 11.00, -12.22, 14.20, -5.20, 17.50])==243\n","Extracted Code:\n","d\n","\n","Test Case: assert cube_Sum(2) == 72\n","Extracted Code:\n","d\n","\n","Test Case: assert concatenate_tuple((\"ID\", \"is\", 4, \"UTS\") ) == 'ID-is-4-UTS'\n","Extracted Code:\n","d\n","\n","Test Case: assert find_Average_Of_Cube(2) == 4.5\n","Extracted Code:\n","N\n","\n","Test Case: assert get_maxgold([[1, 3, 1, 5],[2, 2, 4, 1],[5, 0, 2, 3],[0, 6, 1, 2]],4,4)==16\n","Extracted Code:\n","c\n","\n","Test Case: assert extract_rear(('Mers', 'for', 'Vers') ) == ['s', 'r', 's']\n","Extracted Code:\n","d\n","\n","Test Case: assert count_element_in_list([[1, 3], [5, 7], [1, 11], [1, 15, 7]],1)==3\n","Extracted Code:\n","d\n","\n","Test Case: assert filter_oddnumbers([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])==[1,3,5,7,9]\n","Extracted Code:\n","d\n","\n","Test Case: assert change_date_format(\"2026-01-02\") == '02-01-2026'\n","Extracted Code:\n","d\n","\n","Test Case: assert shell_sort([12, 23, 4, 5, 3, 2, 12, 81, 56, 95]) == [2, 3, 4, 5, 12, 12, 23, 56, 81, 95]\n","Extracted Code:\n","d\n","\n","Test Case: assert and_tuples((10, 4, 6, 9), (5, 2, 3, 3)) == (0, 0, 2, 1)\n","Extracted Code:\n","d\n","\n","Test Case: assert parabola_directrix(5,3,2)==-198\n","Extracted Code:\n","d\n","\n","Test Case: assert common_element([1,2,3,4,5], [5,6,7,8,9])==True\n","Extracted Code:\n","d\n","\n","Test Case: assert median_trapezium(15,25,35)==20\n","Extracted Code:\n","d\n","\n","Test Case: assert check_greater([1, 2, 3, 4, 5], 4) == 'No, entered number is less than those in the array'\n","Extracted Code:\n","d\n","\n","Test Case: assert text_match_one(\"ac\")==('Not matched!')\n","Extracted Code:\n","d\n","\n","Test Case: assert last_Digit(123) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert neg_nos([-1,4,5,-6]) == -1,-6\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_odd(\"python\")==(\"yhn\")\n","Extracted Code:\n","d\n","\n","Test Case: assert count_bidirectional([(5, 6), (1, 2), (6, 5), (9, 1), (6, 5), (2, 1)] ) == '3'\n","Extracted Code:\n","c\n","\n","Test Case: assert multiple_to_single([11, 33, 50])==113350\n","Extracted Code:\n","d\n","\n","Test Case: assert find_adverb_position(\"clearly!! we can see the sky\")==(0, 7, 'clearly')\n","Extracted Code:\n","d\n","\n","Test Case: assert surfacearea_cube(5)==150\n","Extracted Code:\n","d\n","\n","Test Case: assert positive_count([0, 1, 2, -1, -5, 6, 0, -3, -2, 3, 4, 6, 8])==0.54\n","Extracted Code:\n","d\n","\n","Test Case: assert largest_neg([1,2,3,-4,-6]) == -6\n","Extracted Code:\n","d\n","\n","Test Case: assert trim_tuple([(5, 3, 2, 1, 4), (3, 4, 9, 2, 1),(9, 1, 2, 3, 5), (4, 8, 2, 1, 7)], 2) == '[(2,), (9,), (2,), (2,)]'\n","Extracted Code:\n","c\n","\n","Test Case: assert index_multiplication(((1, 3), (4, 5), (2, 9), (1, 10)),((6, 7), (3, 9), (1, 1), (7, 3)) ) == ((6, 21), (12, 45), (2, 9), (7, 30))\n","Extracted Code:\n","d\n","\n","Test Case: assert count_Occurrence(('a', 'a', 'c', 'b', 'd'),['a', 'b'] ) == 3\n","Extracted Code:\n","d\n","\n","Test Case: assert cube_nums([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])==[1, 8, 27, 64, 125, 216, 343, 512, 729, 1000]\n","Extracted Code:\n","d\n","\n","Test Case: assert cal_sum(9) == 49\n","Extracted Code:\n","d\n","\n","Test Case: assert check_Triangle(1,5,2,5,4,6) == 'Yes'\n","Extracted Code:\n","d\n","\n","Test Case: assert extract_string(['Python', 'list', 'exercises', 'practice', 'solution'] ,8)==['practice', 'solution']\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_whitespaces(' Google    Flutter ') == 'GoogleFlutter'\n","Extracted Code:\n","c\n","\n","Test Case: assert loss_amount(1500,1200)==None\n","Extracted Code:\n","d\n","\n","Test Case: assert sumofFactors(18) == 26\n","Extracted Code:\n","d\n","\n","Test Case: assert text_match_wordz(\"pythonz.\")==('Found a match!')\n","Extracted Code:\n","d\n","\n","Test Case: assert check_monthnumb_number(5)==True\n","Extracted Code:\n","d\n","\n","Test Case: assert reverse_string_list(['Red', 'Green', 'Blue', 'White', 'Black'])==['deR', 'neerG', 'eulB', 'etihW', 'kcalB']\n","Extracted Code:\n","d\n","\n","Test Case: assert Find_Min([[1],[1,2],[1,2,3]]) == [1]\n","Extracted Code:\n","d\n","\n","Test Case: assert rectangle_area(10,20)==200\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_uppercase('cAstyoUrFavoRitETVshoWs') == 'cstyoravoitshos'\n","Extracted Code:\n","N\n","\n","Test Case: assert Extract([[1, 2], [3, 4, 5], [6, 7, 8, 9]]) == [1, 3, 6]\n","Extracted Code:\n","d\n","\n","Test Case: assert upper_ctr('PYthon') == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert combinations_list(['orange', 'red', 'green', 'blue'])==[[], ['orange'], ['red'], ['red', 'orange'], ['green'], ['green', 'orange'], ['green', 'red'], ['green', 'red', 'orange'], ['blue'], ['blue', 'orange'], ['blue', 'red'], ['blue', 'red', 'orange'], ['blue', 'green'], ['blue', 'green', 'orange'], ['blue', 'green', 'red'], ['blue', 'green', 'red', 'orange']]\n","Extracted Code:\n","N\n","\n","Test Case: assert max_subarray_product([1, -2, -3, 0, 7, -8, -2]) == 112\n","Extracted Code:\n","c\n","\n","Test Case: assert check_value({'Cierra Vega': 12, 'Alden Cantrell': 12, 'Kierra Gentry': 12, 'Pierre Cox': 12},10)==False\n","Extracted Code:\n","d\n","\n","Test Case: assert drop_empty({'c1': 'Red', 'c2': 'Green', 'c3':None})=={'c1': 'Red', 'c2': 'Green'}\n","Extracted Code:\n","d\n","\n","Test Case: assert find_peak([1, 3, 20, 4, 1, 0], 6) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert decimal_to_Octal(10) == 12\n","Extracted Code:\n","d\n","\n","Test Case: assert max_product([3, 100, 4, 5, 150, 6], 6) == 45000 \n","Extracted Code:\n","d\n","\n","Test Case: assert max_profit([1, 5, 2, 3, 7, 6, 4, 5], 3) == 10\n","Extracted Code:\n","c\n","\n","Test Case: assert add_pairwise((1, 5, 7, 8, 10)) == (6, 12, 15, 18)\n","Extracted Code:\n","d\n","\n","Test Case: assert find_remainder([ 100, 10, 5, 25, 35, 14 ],6,11) ==9\n","Extracted Code:\n","d\n","\n","Test Case: assert check_Consecutive([1,2,3,4,5]) == True\n","Extracted Code:\n","N\n","\n","Test Case: assert tuple_intersection([(3, 4), (5, 6), (9, 10), (4, 5)] , [(5, 4), (3, 4), (6, 5), (9, 11)]) == {(4, 5), (3, 4), (5, 6)}\n","Extracted Code:\n","d\n","\n","Test Case: assert replace_char(\"polygon\",'y','l')==(\"pollgon\")\n","Extracted Code:\n","d\n","\n","Test Case: assert sort_counter({'Math':81, 'Physics':83, 'Chemistry':87})==[('Chemistry', 87), ('Physics', 83), ('Math', 81)]\n","Extracted Code:\n","d\n","\n","Test Case: assert big_sum([1,2,3]) == 4\n","Extracted Code:\n","d\n","\n","Test Case: assert is_lower(\"InValid\") == \"invalid\"\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_lowercase(\"PYTHon\")==('PYTH')\n","Extracted Code:\n","d\n","\n","Test Case: assert first_Digit(123) == 1\n","Extracted Code:\n","N\n","\n","Test Case: assert get_max_occuring_char(\"data\") == \"a\"\n","Extracted Code:\n","d\n","\n","Test Case: assert is_subset_sum([3, 34, 4, 12, 5, 2], 6, 9) == True\n","Extracted Code:\n","d\n","\n","Test Case: assert match(\"Geeks\") == 'Yes'\n","Extracted Code:\n","d\n","\n","Test Case: assert first_Factorial_Divisible_Number(10) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_matching_tuple([('Hello', 'dude'), ('How', 'are'), ('you', '?')], [('Hello', 'dude'), ('How', 'are')]) == [('you', '?')]\n","Extracted Code:\n","d\n","\n","Test Case: assert largest_palindrome([1, 232, 54545, 999991], 4) == 54545\n","Extracted Code:\n","d\n","\n","Test Case: assert binomial_probability(10, 5, 1.0/3) == 0.13656454808718185\n","Extracted Code:\n","d\n","\n","Test Case: assert sort_tuple([(1, 3), (3, 2), (2, 1)] ) == [(2, 1), (3, 2), (1, 3)]\n","Extracted Code:\n","d\n","\n","Test Case: assert area_pentagon(5)==43.01193501472417\n","Extracted Code:\n","d\n","\n","Test Case: assert frequency_Of_Largest(5,[1,2,3,4,4]) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert extract_symmetric([(6, 7), (2, 3), (7, 6), (9, 8), (10, 2), (8, 9)] ) == {(8, 9), (6, 7)}\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_gp(1,5,2)==31\n","Extracted Code:\n","d\n","\n","Test Case: assert binary_search([1,2,3,5,8], 6) == False\n","Extracted Code:\n","d\n","\n","Test Case: assert calculate_polygons(1,1, 4, 4, 3)==[[(-5.0, -4.196152422706632), (-5.0, -0.7320508075688767), (-2.0, 1.0), (1.0, -0.7320508075688767), (1.0, -4.196152422706632), (-2.0, -5.928203230275509), (-5.0, -4.196152422706632)], [(1.0, -4.196152422706632), (1.0, -0.7320508075688767), (4.0, 1.0), (7.0, -0.7320508075688767), (7.0, -4.196152422706632), (4.0, -5.928203230275509), (1.0, -4.196152422706632)], [(7.0, -4.196152422706632), (7.0, -0.7320508075688767), (10.0, 1.0), (13.0, -0.7320508075688767), (13.0, -4.196152422706632), (10.0, -5.928203230275509), (7.0, -4.196152422706632)], [(-2.0, 1.0000000000000004), (-2.0, 4.464101615137755), (1.0, 6.196152422706632), (4.0, 4.464101615137755), (4.0, 1.0000000000000004), (1.0, -0.7320508075688767), (-2.0, 1.0000000000000004)], [(4.0, 1.0000000000000004), (4.0, 4.464101615137755), (7.0, 6.196152422706632), (10.0, 4.464101615137755), (10.0, 1.0000000000000004), (7.0, -0.7320508075688767), (4.0, 1.0000000000000004)], [(-5.0, 6.196152422706632), (-5.0, 9.660254037844387), (-2.0, 11.392304845413264), (1.0, 9.660254037844387), (1.0, 6.196152422706632), (-2.0, 4.464101615137755), (-5.0, 6.196152422706632)], [(1.0, 6.196152422706632), (1.0, 9.660254037844387), (4.0, 11.392304845413264), (7.0, 9.660254037844387), (7.0, 6.196152422706632), (4.0, 4.464101615137755), (1.0, 6.196152422706632)], [(7.0, 6.196152422706632), (7.0, 9.660254037844387), (10.0, 11.392304845413264), (13.0, 9.660254037844387), (13.0, 6.196152422706632), (10.0, 4.464101615137755), (7.0, 6.196152422706632)], [(-2.0, 11.392304845413264), (-2.0, 14.85640646055102), (1.0, 16.588457268119896), (4.0, 14.85640646055102), (4.0, 11.392304845413264), (1.0, 9.660254037844387), (-2.0, 11.392304845413264)], [(4.0, 11.392304845413264), (4.0, 14.85640646055102), (7.0, 16.588457268119896), (10.0, 14.85640646055102), (10.0, 11.392304845413264), (7.0, 9.660254037844387), (4.0, 11.392304845413264)]]\n","Extracted Code:\n","a\n","\n","Test Case: assert binary_to_integer((1, 1, 0, 1, 0, 0, 1)) == '105'\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_lowercase('KDeoALOklOOHserfLoAJSIskdsf') == 'KDALOOOHLAJSI'\n","Extracted Code:\n","d\n","\n","Test Case: assert heap_queue_smallest( [25, 35, 22, 85, 14, 65, 75, 25, 58],3)==[14, 22, 25] \n","Extracted Code:\n","d\n","\n","Test Case: assert surfacearea_cone(5,12)==282.7433388230814\n","Extracted Code:\n","d\n","\n","Test Case: assert gcd(12, 17) == 1\n","Extracted Code:\n","d\n","\n","Test Case: assert diameter_circle(10)==20\n","Extracted Code:\n","d\n","\n","Test Case: assert concatenate_elements(['hello','there','have','a','rocky','day'] ) == '  hello there have a rocky day'\n","Extracted Code:\n","d\n","\n","Test Case: assert num_comm_div(2,4) == 2\n","Extracted Code:\n","d\n","\n","Test Case: assert find(3,3) == 0\n","Extracted Code:\n","d\n","\n","Test Case: assert add_consecutive_nums([1, 1, 3, 4, 4, 5, 6, 7])==[2, 4, 7, 8, 9, 11, 13]\n","Extracted Code:\n","d\n","\n","Test Case: assert sum_Of_Series(5) == 225\n","Extracted Code:\n","d\n","\n","Test Case: assert re_order([6, 0, 8, 2, 3, 0, 4, 0, 1]) == [6, 8, 2, 3, 4, 1, 0, 0, 0]\n","Extracted Code:\n","d\n","\n","Test Case: assert permutation_coefficient(10, 2) == 90\n","Extracted Code:\n","d\n","\n","Test Case: assert remove_words(['red', 'green', 'blue', 'white', 'black', 'orange'],['white', 'orange'])==['red', 'green', 'blue', 'black']\n","Extracted Code:\n","d\n","\n","Test Case: assert same_order([\"red\",\"green\",\"black\",\"orange\"],[\"red\",\"pink\",\"green\",\"white\",\"black\"])==True\n","Extracted Code:\n","d\n","\n","Test Case: assert average_Odd(9) == 5\n","Extracted Code:\n","d\n","\n","Test Case: assert no_of_subsequences([1,2,3,4], 10) == 11\n","Extracted Code:\n","N\n","\n"]}],"source":["import re\n","import json\n","\n","# Define the regex pattern to extract code between \"### Response:\" and the first <eos>\n","pattern = re.compile(r\"### Response:\\s*(.*?)<eos>\", re.DOTALL)\n","\n","# Define the checkpoint frequency\n","checkpoint_frequency = 10\n","\n","# Load previous results if a checkpoint file exists\n","file = \"results.json\"\n","try:\n","    with open(file, \"r\") as f:\n","        results = json.load(f)\n","except FileNotFoundError:\n","    results = []\n","\n","# Determine the starting index based on the length of the existing results\n","start_index = len(results)\n","\n","for i in range(start_index, len(test_dataset['text'])):\n","    instruction = test_dataset['text'][i]\n","    test_case = test_dataset['test_list'][i][0]\n","    test_cases = test_dataset['test_list'][i]\n","    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n","\n","    inputs = tokenizer(\n","        [\n","            alpaca_prompt.format(\n","                instruction,  # instruction\n","                test_case,\n","                \"\",  # output - leave this blank for generation!\n","            )\n","        ], return_tensors=\"pt\"\n","    ).to(\"cuda\")\n","\n","    outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True,)\n","\n","    generated_text = tokenizer.batch_decode(outputs)[0]\n","\n","    # Find all matches of the pattern in the generated output\n","    matches = pattern.findall(generated_text)\n","\n","    # Extract the first match which is the desired code block\n","    if matches:\n","        extracted_code = matches[0].strip()\n","    else:\n","        extracted_code = \"No code block found.\"\n","\n","    # Store the test case and extracted code as a pair\n","    results.append((test_cases, extracted_code))\n","\n","    # Save the checkpoint every 10 iterations\n","    if (i + 1) % checkpoint_frequency == 0:\n","        with open(file, \"w\") as f:\n","            json.dump(results, f, indent=4)\n","        print(f\"Checkpoint saved at index {i + 1}\")\n","\n","# Final save after the loop ends\n","with open(file, \"w\") as f:\n","    json.dump(results, f, indent=4)\n","print(\"Final checkpoint saved.\")\n","\n","# Display or save the results\n","for test_case, code in results:\n","    print(f\"Test Case: {test_case[0]}\\nExtracted Code:\\n{code[0]}\\n\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lf3pJbzGwas","outputId":"253c5852-823a-4497-baf6-3150b48932bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["def sort_matrix(matrix):\n","\trow_sum = []\n","\tfor index, row in enumerate(matrix):\n","\t\trow_sum.append(sum(row))\n","\tmatrix.sort(key = lambda x: row_sum[matrix.index(x)])\n","\trow_sum.sort()# to get the index of the next smallest sum\n","\tfor index, row in enumerate(matrix):\n","\t\tif row_sum[index] == row_sum[index - 1]:\n","\t\t\t\tmatrix.remove(matrix[index - 1])\n","\t\t\t\tmatrix.insert(index - 1, matrix[-1])\n","\t\t\t\tmatrix.remove(matrix[-1])\n","\treturn matrix\n"]}],"source":["print(\"def sort_matrix(matrix):\\r\\n\\trow_sum = []\\r\\n\\tfor index, row in enumerate(matrix):\\r\\n\\t\\trow_sum.append(sum(row))\\r\\n\\tmatrix.sort(key = lambda x: row_sum[matrix.index(x)])\\r\\n\\trow_sum.sort()# to get the index of the next smallest sum\\r\\n\\tfor index, row in enumerate(matrix):\\r\\n\\t\\tif row_sum[index] == row_sum[index - 1]:\\r\\n\\t\\t\\t\\tmatrix.remove(matrix[index - 1])\\r\\n\\t\\t\\t\\tmatrix.insert(index - 1, matrix[-1])\\r\\n\\t\\t\\t\\tmatrix.remove(matrix[-1])\\r\\n\\treturn matrix\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ff73c7aaa91d42c59a6e8fc6dc20fd78":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d520b15be0d24cc78c95014dcf6b0e4e","IPY_MODEL_e4dc289256e2454ea124fd2f94a219a9","IPY_MODEL_b80bc51b209a45ba97991563a63ed266"],"layout":"IPY_MODEL_393948ac709e425e82aa4cb7cc20e2b1"}},"d520b15be0d24cc78c95014dcf6b0e4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3d7925c4d834dcda23820758b76c4ed","placeholder":"​","style":"IPY_MODEL_7a9933e5437b4808b1fe8e588d78be73","value":"config.json: 100%"}},"e4dc289256e2454ea124fd2f94a219a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63f92702c6944bccb5e280f2db14099e","max":1156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9619737fc5c445e895208da566bb78c5","value":1156}},"b80bc51b209a45ba97991563a63ed266":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f82943104a2b4cd5965561bc8ba06295","placeholder":"​","style":"IPY_MODEL_3c27f78b904f4965940d69218fdc8a2a","value":" 1.16k/1.16k [00:00&lt;00:00, 84.2kB/s]"}},"393948ac709e425e82aa4cb7cc20e2b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3d7925c4d834dcda23820758b76c4ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a9933e5437b4808b1fe8e588d78be73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63f92702c6944bccb5e280f2db14099e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9619737fc5c445e895208da566bb78c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f82943104a2b4cd5965561bc8ba06295":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c27f78b904f4965940d69218fdc8a2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef2caf141f4349fb9f4b1dcc04dc3138":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5332fa3aad6c45158c0345b9e71c8d7c","IPY_MODEL_6541b8aa027345249b0fd59855748046","IPY_MODEL_8d9daf91332f4137be3f940138cc671b"],"layout":"IPY_MODEL_18c48600ee9c497597d0cb4d756c7d8e"}},"5332fa3aad6c45158c0345b9e71c8d7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc3734ded12b48afb1de1622f93a0949","placeholder":"​","style":"IPY_MODEL_b2b32ebfb07b48478390a4ba837ed753","value":"model.safetensors: 100%"}},"6541b8aa027345249b0fd59855748046":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3b7bf607a23468990aaccfe3b899e3b","max":2264298478,"min":0,"orientation":"horizontal","style":"IPY_MODEL_095db7bb0bda4d9e8636650b71b17daf","value":2264298478}},"8d9daf91332f4137be3f940138cc671b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8ffdff543e146b0bd4a0cb10f042eff","placeholder":"​","style":"IPY_MODEL_7a8145f311ea41139216cd23fba73240","value":" 2.26G/2.26G [00:05&lt;00:00, 439MB/s]"}},"18c48600ee9c497597d0cb4d756c7d8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc3734ded12b48afb1de1622f93a0949":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2b32ebfb07b48478390a4ba837ed753":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3b7bf607a23468990aaccfe3b899e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"095db7bb0bda4d9e8636650b71b17daf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8ffdff543e146b0bd4a0cb10f042eff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a8145f311ea41139216cd23fba73240":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f510cd61b20b4595aa284f156943ee6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4cb8d2f8a5946cda51beb352a88991f","IPY_MODEL_5f8f0a2374b643548cb65644aa577229","IPY_MODEL_e24ac53c19bf4ae6b6df0e1f88b99b06"],"layout":"IPY_MODEL_f2ab2556362447e4b072c2b4490d46d7"}},"d4cb8d2f8a5946cda51beb352a88991f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5740eeaf11cf4aa5989a4882b716c8ab","placeholder":"​","style":"IPY_MODEL_a4bd50bc9a624e71a766974770ccc5d3","value":"generation_config.json: 100%"}},"5f8f0a2374b643548cb65644aa577229":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd6ace092a2544cd876cd9ec7f430c5a","max":172,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63d9ddb6f8de48c694b1562ad5197ab0","value":172}},"e24ac53c19bf4ae6b6df0e1f88b99b06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b53a58944dbf45c7a9cf885bf9894524","placeholder":"​","style":"IPY_MODEL_2b4f5d808e494e48b4405db24e610020","value":" 172/172 [00:00&lt;00:00, 14.9kB/s]"}},"f2ab2556362447e4b072c2b4490d46d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5740eeaf11cf4aa5989a4882b716c8ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4bd50bc9a624e71a766974770ccc5d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd6ace092a2544cd876cd9ec7f430c5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63d9ddb6f8de48c694b1562ad5197ab0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b53a58944dbf45c7a9cf885bf9894524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b4f5d808e494e48b4405db24e610020":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51925c33bb384c4a9e1c3cec1c7f44af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f616448239c240eabc05b0b13c6d04ed","IPY_MODEL_b7dd4a3df6d246a9813b017e94bc4cee","IPY_MODEL_8ba1497dd5e440ed858569e115c1182c"],"layout":"IPY_MODEL_315b21fecd9a45babb7aa54c9a2d9cb7"}},"f616448239c240eabc05b0b13c6d04ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b51255d569946c492f74feb5f078465","placeholder":"​","style":"IPY_MODEL_7ff29d6f292d4bbd9196519fd65c62ee","value":"tokenizer_config.json: 100%"}},"b7dd4a3df6d246a9813b017e94bc4cee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0333f11b277466c93b8ae64d3ebe462","max":3172,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b52559c9a6144fccb78cf8c6ca6ff07f","value":3172}},"8ba1497dd5e440ed858569e115c1182c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_279f6e84c91145be9bf6cd217eb4ed9d","placeholder":"​","style":"IPY_MODEL_c15a6ed7b1cb4f1c97c6100b2ffa19ae","value":" 3.17k/3.17k [00:00&lt;00:00, 265kB/s]"}},"315b21fecd9a45babb7aa54c9a2d9cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b51255d569946c492f74feb5f078465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ff29d6f292d4bbd9196519fd65c62ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0333f11b277466c93b8ae64d3ebe462":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b52559c9a6144fccb78cf8c6ca6ff07f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"279f6e84c91145be9bf6cd217eb4ed9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c15a6ed7b1cb4f1c97c6100b2ffa19ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"329122862252485f91435e533bd1f3fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36a71747beac4c37aaa43e49723920c4","IPY_MODEL_3f9581d535c44d04979035e325bea57d","IPY_MODEL_2b77364d071a4399a2aa0935153a5ebe"],"layout":"IPY_MODEL_2ee8d52ca37b4674a6151a9086ca210f"}},"36a71747beac4c37aaa43e49723920c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_771d167b06d14d4093e945fb41e2e0af","placeholder":"​","style":"IPY_MODEL_a25d07749b2b42bfb5c5a44a400c5be1","value":"tokenizer.model: 100%"}},"3f9581d535c44d04979035e325bea57d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4371118dcdd48b5b3e3d99cc65c8dcb","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b0e031c85a7496599f0b00c3c14db69","value":499723}},"2b77364d071a4399a2aa0935153a5ebe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d18e1487cf9436c8aba1d9ea11ece77","placeholder":"​","style":"IPY_MODEL_f623c036b7ad4f099deb4d61e31918d6","value":" 500k/500k [00:00&lt;00:00, 27.5MB/s]"}},"2ee8d52ca37b4674a6151a9086ca210f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"771d167b06d14d4093e945fb41e2e0af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a25d07749b2b42bfb5c5a44a400c5be1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4371118dcdd48b5b3e3d99cc65c8dcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b0e031c85a7496599f0b00c3c14db69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d18e1487cf9436c8aba1d9ea11ece77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f623c036b7ad4f099deb4d61e31918d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff3bf4c47650467b901e6fa8f92c7783":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_030f5f4701704fa0847b11404163893f","IPY_MODEL_075ab2040b0a4273bb26337e3ec5e690","IPY_MODEL_6cf8cb82e35f4d05bd8eae1092291395"],"layout":"IPY_MODEL_8e0ed61be4264d41947c33bfff79f423"}},"030f5f4701704fa0847b11404163893f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bca29ef67d644dca17ab66317670e1e","placeholder":"​","style":"IPY_MODEL_244dc21f57e44a089cb4ba734fc67a09","value":"added_tokens.json: 100%"}},"075ab2040b0a4273bb26337e3ec5e690":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_607db43475cb497fa5bba38de38ac654","max":293,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1af4b37bceb14ac8950dd0d34e234c0a","value":293}},"6cf8cb82e35f4d05bd8eae1092291395":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_551d0ebc36fc4b3b83b9ae4fd6b17316","placeholder":"​","style":"IPY_MODEL_f6e0287731ba45f6b939915120f60ad1","value":" 293/293 [00:00&lt;00:00, 18.1kB/s]"}},"8e0ed61be4264d41947c33bfff79f423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bca29ef67d644dca17ab66317670e1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"244dc21f57e44a089cb4ba734fc67a09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"607db43475cb497fa5bba38de38ac654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1af4b37bceb14ac8950dd0d34e234c0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"551d0ebc36fc4b3b83b9ae4fd6b17316":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6e0287731ba45f6b939915120f60ad1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c562645c20d4f49bac008da9f72aac1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bfc47f69c4b482584c830fecef2d148","IPY_MODEL_e30d2b9b7cd945acb7ef00973f910eb2","IPY_MODEL_a13cff2d215c4747aa891552f6d2746d"],"layout":"IPY_MODEL_b28875a4cc9246e99b3eadc0003104d9"}},"0bfc47f69c4b482584c830fecef2d148":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3529180d681248dc89f3d1c61e5269d8","placeholder":"​","style":"IPY_MODEL_d396e4da70cb4b0c9e72a07c8430c17f","value":"special_tokens_map.json: 100%"}},"e30d2b9b7cd945acb7ef00973f910eb2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0db537b9d0e7401baf0f8335cdcb4603","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf75a0ecaa664b4fb39d7e872a64eabb","value":571}},"a13cff2d215c4747aa891552f6d2746d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1b46b0baed3405abb91efbf392041f2","placeholder":"​","style":"IPY_MODEL_75bd2d9b8b1348a1bdc673e9aacb3db8","value":" 571/571 [00:00&lt;00:00, 49.2kB/s]"}},"b28875a4cc9246e99b3eadc0003104d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3529180d681248dc89f3d1c61e5269d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d396e4da70cb4b0c9e72a07c8430c17f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0db537b9d0e7401baf0f8335cdcb4603":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf75a0ecaa664b4fb39d7e872a64eabb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1b46b0baed3405abb91efbf392041f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75bd2d9b8b1348a1bdc673e9aacb3db8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c25d82134ed446aba34f2b70f7ece02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0683223056247d29851e7afb2a53d03","IPY_MODEL_e1ca433b561d4981b72792c0394205ea","IPY_MODEL_ac4a298d39cd471c9b084f5499166c44"],"layout":"IPY_MODEL_715a22b4d22b4f1dbf2b33024f16e7c8"}},"a0683223056247d29851e7afb2a53d03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e190b5de92c40baa7a625d028eb0748","placeholder":"​","style":"IPY_MODEL_729bbd6704d443b88aa56aa6733bc65a","value":"tokenizer.json: 100%"}},"e1ca433b561d4981b72792c0394205ea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3d73b89a0694cb190550229b490510d","max":1844868,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e39ca33a3d64683bd4ec5355608f8b9","value":1844868}},"ac4a298d39cd471c9b084f5499166c44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cef1f59c964b4309866a91ddb7a323a4","placeholder":"​","style":"IPY_MODEL_7889a111a3984d8b8181fe748dbdacb8","value":" 1.84M/1.84M [00:00&lt;00:00, 3.85MB/s]"}},"715a22b4d22b4f1dbf2b33024f16e7c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e190b5de92c40baa7a625d028eb0748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"729bbd6704d443b88aa56aa6733bc65a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3d73b89a0694cb190550229b490510d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e39ca33a3d64683bd4ec5355608f8b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cef1f59c964b4309866a91ddb7a323a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7889a111a3984d8b8181fe748dbdacb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b823e9e45164ef58b07bde79760bca7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ebf3399593154b0faf39fb473a87e421","IPY_MODEL_c00ca5f8c2464347abf91bbfb597de55","IPY_MODEL_e884c9fb585045d4888f0447d7fedf7a"],"layout":"IPY_MODEL_443e415ca87043ac9a7f46c7ba67063b"}},"ebf3399593154b0faf39fb473a87e421":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1bdd55fe3e3425a972e3d8122ae988e","placeholder":"​","style":"IPY_MODEL_03f03c2030c44621b02df2f50f57e24b","value":"Downloading readme: 100%"}},"c00ca5f8c2464347abf91bbfb597de55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78edb7885db94b0a95d00f3e8b5d5c01","max":9058,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bb2cf3a8c5d4bb0923e38e95aa83277","value":9058}},"e884c9fb585045d4888f0447d7fedf7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18900b4b112045658e239a8e64f66673","placeholder":"​","style":"IPY_MODEL_e03c120d3f904204ad2c9117865d9de3","value":" 9.06k/9.06k [00:00&lt;00:00, 735kB/s]"}},"443e415ca87043ac9a7f46c7ba67063b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1bdd55fe3e3425a972e3d8122ae988e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03f03c2030c44621b02df2f50f57e24b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78edb7885db94b0a95d00f3e8b5d5c01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bb2cf3a8c5d4bb0923e38e95aa83277":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18900b4b112045658e239a8e64f66673":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e03c120d3f904204ad2c9117865d9de3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65e2a305aed547e48298c68e1f892eca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df54b038c0d94595bcf14c8cca75df76","IPY_MODEL_1b057050db8c4f998e5f676c87f49484","IPY_MODEL_9ee83c505672479eb703cb87219cad58"],"layout":"IPY_MODEL_7d8c8e3c4a7d45228b154ee9b649374b"}},"df54b038c0d94595bcf14c8cca75df76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4313b0efb6eb4d78af92b3c7834751db","placeholder":"​","style":"IPY_MODEL_881da92665664b33962905ed2c3dca28","value":"Downloading data: 100%"}},"1b057050db8c4f998e5f676c87f49484":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cbde20ad68140f8809c7dc7e05a8c1c","max":87223,"min":0,"orientation":"horizontal","style":"IPY_MODEL_491a295bc9624c2dab567d29913a658d","value":87223}},"9ee83c505672479eb703cb87219cad58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6acd59be62ed41d58a30b34f85be961c","placeholder":"​","style":"IPY_MODEL_a7c134bce0c946408a7330b791d10323","value":" 87.2k/87.2k [00:01&lt;00:00, 66.4kB/s]"}},"7d8c8e3c4a7d45228b154ee9b649374b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4313b0efb6eb4d78af92b3c7834751db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"881da92665664b33962905ed2c3dca28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7cbde20ad68140f8809c7dc7e05a8c1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"491a295bc9624c2dab567d29913a658d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6acd59be62ed41d58a30b34f85be961c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7c134bce0c946408a7330b791d10323":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"708c6617df3c4652bf74e542790a640e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a9999f10c3241cdaa2e8c4686416d2c","IPY_MODEL_5f842bb260794ba48dd6ec2d849fa26e","IPY_MODEL_25077d3e29c542fe969cbf2c1257df07"],"layout":"IPY_MODEL_2d2fa26f06764510b0776715d9bc9645"}},"5a9999f10c3241cdaa2e8c4686416d2c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41841770156c48ddbf3dd7d220a16e88","placeholder":"​","style":"IPY_MODEL_150585bbfc804cbe8eba0a8c581e2ffa","value":"Downloading data: 100%"}},"5f842bb260794ba48dd6ec2d849fa26e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9ba483a7ffc4e0cabf1f4331c7761a0","max":115824,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8db0d6b58354e6b8a5417ae98deed12","value":115824}},"25077d3e29c542fe969cbf2c1257df07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8eacf0103c044dbbb8df8b0951c0e30d","placeholder":"​","style":"IPY_MODEL_c8304cd0dd9448ae8d41e4065c7ac4c4","value":" 116k/116k [00:01&lt;00:00, 90.6kB/s]"}},"2d2fa26f06764510b0776715d9bc9645":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41841770156c48ddbf3dd7d220a16e88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"150585bbfc804cbe8eba0a8c581e2ffa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9ba483a7ffc4e0cabf1f4331c7761a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8db0d6b58354e6b8a5417ae98deed12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8eacf0103c044dbbb8df8b0951c0e30d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8304cd0dd9448ae8d41e4065c7ac4c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"934bbfcfb7fb400d96f776c413556bbf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e315388f3fe344cea7f83471be15a0fe","IPY_MODEL_6af68ec870ad407988a98b35cc009449","IPY_MODEL_88b1655ff77f4c3dbebe2170331a8684"],"layout":"IPY_MODEL_b84defb5396848cf80400c6551386506"}},"e315388f3fe344cea7f83471be15a0fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a409ef9816c4dea9b2a6ef85800f18f","placeholder":"​","style":"IPY_MODEL_d4f0411925b7436da65507b3a41de82a","value":"Downloading data: 100%"}},"6af68ec870ad407988a98b35cc009449":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1c8c7abfa2541f4afe32e2d4de0a0f4","max":25144,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4ab1c08d1474b7a9b76f04ec098acba","value":25144}},"88b1655ff77f4c3dbebe2170331a8684":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b6b276b399b498e8d231ed7bb6312ac","placeholder":"​","style":"IPY_MODEL_5543b9d96f8343dcb7d2d607af2af124","value":" 25.1k/25.1k [00:01&lt;00:00, 18.1kB/s]"}},"b84defb5396848cf80400c6551386506":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a409ef9816c4dea9b2a6ef85800f18f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4f0411925b7436da65507b3a41de82a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1c8c7abfa2541f4afe32e2d4de0a0f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4ab1c08d1474b7a9b76f04ec098acba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b6b276b399b498e8d231ed7bb6312ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5543b9d96f8343dcb7d2d607af2af124":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ab91447ac0f47749d7ed7da9135796d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86c4f0a181e44c1c860ec30345fac8d7","IPY_MODEL_0f9e567e728347ffaaf3e237c715f177","IPY_MODEL_8c93974a7c584c2ab69efeab5d404d93"],"layout":"IPY_MODEL_a59c679f65364ba5b23d36ee2ea71359"}},"86c4f0a181e44c1c860ec30345fac8d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64bb65a13d084f2c85e4cf23a648fa20","placeholder":"​","style":"IPY_MODEL_904359496d944ebc8240a884a40578b8","value":"Downloading data: 100%"}},"0f9e567e728347ffaaf3e237c715f177":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a446bfc61e948d4bc5dd7c6ded60dbf","max":7878,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf7dd10fdd944e92ad43b68101f48e66","value":7878}},"8c93974a7c584c2ab69efeab5d404d93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_feb26b7226484dd4b61372dae94d9f82","placeholder":"​","style":"IPY_MODEL_a135b0b5fe454338b5166087bb5377b6","value":" 7.88k/7.88k [00:01&lt;00:00, 6.02kB/s]"}},"a59c679f65364ba5b23d36ee2ea71359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64bb65a13d084f2c85e4cf23a648fa20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"904359496d944ebc8240a884a40578b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a446bfc61e948d4bc5dd7c6ded60dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf7dd10fdd944e92ad43b68101f48e66":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"feb26b7226484dd4b61372dae94d9f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a135b0b5fe454338b5166087bb5377b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c3a8de2959643f792346b7c06989ad4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fea06c53d62446e5ab36c535fe74fe7b","IPY_MODEL_da93b100650142bc9b3fca1e0b5be1b1","IPY_MODEL_18a9d70146a54f479f1a2d96826ffb86"],"layout":"IPY_MODEL_54ea885cb2a44adb8e98de3bd5bfa3f8"}},"fea06c53d62446e5ab36c535fe74fe7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5d0d485f92b486e9d482d6fd187f330","placeholder":"​","style":"IPY_MODEL_95c6e20578994042b0b6bf1abd6149da","value":"Generating train split: 100%"}},"da93b100650142bc9b3fca1e0b5be1b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d41655ca913b409fa2e176d61f4c9f0e","max":374,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4622415bc854189b6e9cd26670bebb1","value":374}},"18a9d70146a54f479f1a2d96826ffb86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7e7aef4614942068556f842ad33e1e5","placeholder":"​","style":"IPY_MODEL_9e2f512ef3884d00938000f26eda9c65","value":" 374/374 [00:00&lt;00:00, 8167.30 examples/s]"}},"54ea885cb2a44adb8e98de3bd5bfa3f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5d0d485f92b486e9d482d6fd187f330":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95c6e20578994042b0b6bf1abd6149da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d41655ca913b409fa2e176d61f4c9f0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4622415bc854189b6e9cd26670bebb1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7e7aef4614942068556f842ad33e1e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e2f512ef3884d00938000f26eda9c65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81aa96a2fae94731bf06ca86fa2fdb9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83713a08ae4648d2b231a7f53bdcca8d","IPY_MODEL_551ec9da1998482eac1bbda910092b62","IPY_MODEL_eba2ae5d86ee49f2bf2dab6c51daa065"],"layout":"IPY_MODEL_61b26400e3534c7f8406a1a55c19f907"}},"83713a08ae4648d2b231a7f53bdcca8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f777c064b0d4deaa5baa1828be6b407","placeholder":"​","style":"IPY_MODEL_94eef1729690499ca57683820dbd5741","value":"Generating test split: 100%"}},"551ec9da1998482eac1bbda910092b62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfa45b10e61643a1ae0889b690a369b4","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce91d597e889485b9a05e9eb9cb0f855","value":500}},"eba2ae5d86ee49f2bf2dab6c51daa065":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c688781b68444b58844351aa3c0b6a41","placeholder":"​","style":"IPY_MODEL_79fa3c3b50d343b3b551602de1714f41","value":" 500/500 [00:00&lt;00:00, 26338.19 examples/s]"}},"61b26400e3534c7f8406a1a55c19f907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f777c064b0d4deaa5baa1828be6b407":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94eef1729690499ca57683820dbd5741":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfa45b10e61643a1ae0889b690a369b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce91d597e889485b9a05e9eb9cb0f855":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c688781b68444b58844351aa3c0b6a41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79fa3c3b50d343b3b551602de1714f41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"def3ff17361744d29d2f00afd72b1b90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_359c531d5cff4aff8f3bfffd711ecee3","IPY_MODEL_b4f30d2929544e14abaf8f0036ae00a2","IPY_MODEL_be68e2eff47044909646d00699c8d26a"],"layout":"IPY_MODEL_b6dc5a5c658b46dda5ba9bd8d55346f0"}},"359c531d5cff4aff8f3bfffd711ecee3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de726f613c814d7d94891709d5b879f9","placeholder":"​","style":"IPY_MODEL_6aaf39659c984e72abda911ec3d12414","value":"Generating validation split: 100%"}},"b4f30d2929544e14abaf8f0036ae00a2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b30fd9fcb6c14e479f37c5411ecb86a7","max":90,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52ec704a9ecc40589c11cd9bb2ead791","value":90}},"be68e2eff47044909646d00699c8d26a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19e1c896fa4242ccbdbe2f2a29af33d0","placeholder":"​","style":"IPY_MODEL_7770d502e5d54a95b910c4112a3ef751","value":" 90/90 [00:00&lt;00:00, 3991.15 examples/s]"}},"b6dc5a5c658b46dda5ba9bd8d55346f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de726f613c814d7d94891709d5b879f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aaf39659c984e72abda911ec3d12414":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b30fd9fcb6c14e479f37c5411ecb86a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52ec704a9ecc40589c11cd9bb2ead791":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19e1c896fa4242ccbdbe2f2a29af33d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7770d502e5d54a95b910c4112a3ef751":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c09258496b4d4a0198c86ef193587a8c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9a1cb54c6f5e4cfc948badf0cae27019","IPY_MODEL_d32231e4e905456f9df9f8cf13a2fe3a","IPY_MODEL_fca2c3dcbab24c4c89f2bb69e27b9709"],"layout":"IPY_MODEL_223bd175447448de934a2ce39758efcb"}},"9a1cb54c6f5e4cfc948badf0cae27019":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a0a94c7ad0844579e85d4f0fd05a22f","placeholder":"​","style":"IPY_MODEL_4b10fd994aca4b399b2f34ff6a7f4e1e","value":"Generating prompt split: 100%"}},"d32231e4e905456f9df9f8cf13a2fe3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18a8f3d9130540579164aeafff8cf197","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58b6d2423abb49ca9499e0601d5114c7","value":10}},"fca2c3dcbab24c4c89f2bb69e27b9709":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95e6da2dd4e8410cb1dee56b7f856164","placeholder":"​","style":"IPY_MODEL_27bff28371c04d71a909846dd5c1c047","value":" 10/10 [00:00&lt;00:00, 486.44 examples/s]"}},"223bd175447448de934a2ce39758efcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a0a94c7ad0844579e85d4f0fd05a22f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b10fd994aca4b399b2f34ff6a7f4e1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18a8f3d9130540579164aeafff8cf197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b6d2423abb49ca9499e0601d5114c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95e6da2dd4e8410cb1dee56b7f856164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27bff28371c04d71a909846dd5c1c047":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed98288ea7274895a2a9106cbb6aa6be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_beeea5ade3ee42368d72947db60740b1","IPY_MODEL_4687518616a243f381cea886678fca31","IPY_MODEL_8db4fa4d68a744b6a746d3f17b504c26"],"layout":"IPY_MODEL_e2dc0e27154b47bbb563037c5e3d9738"}},"beeea5ade3ee42368d72947db60740b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_744735a2852c4bbb8dcb225853d5df38","placeholder":"​","style":"IPY_MODEL_db22faef0bd6460b991cf4d6e287af8d","value":"Map: 100%"}},"4687518616a243f381cea886678fca31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9a7706eb62f4d569ef847b77ba9268b","max":374,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a9ba3b4cbd14d7a856317ad05fa42a5","value":374}},"8db4fa4d68a744b6a746d3f17b504c26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_103b5cba1fe74379807234d64ac7ade4","placeholder":"​","style":"IPY_MODEL_8e13c462d9844dfcb4707a47c2b7ef5e","value":" 374/374 [00:00&lt;00:00, 10607.72 examples/s]"}},"e2dc0e27154b47bbb563037c5e3d9738":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744735a2852c4bbb8dcb225853d5df38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db22faef0bd6460b991cf4d6e287af8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9a7706eb62f4d569ef847b77ba9268b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a9ba3b4cbd14d7a856317ad05fa42a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"103b5cba1fe74379807234d64ac7ade4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e13c462d9844dfcb4707a47c2b7ef5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48f49e88ceca4a8998fe3e8b6c8dc6b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cba1915fe1d64af1aa02abc8033ba69c","IPY_MODEL_9611fa78c64242108da5730b0415cacf","IPY_MODEL_8c6a28bfda0d452585d01edbbe26a2f8"],"layout":"IPY_MODEL_b50dd8fe90194116bc9a75b83466d1c6"}},"cba1915fe1d64af1aa02abc8033ba69c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68ff6ddb260749b6934db8a4e81bbb4b","placeholder":"​","style":"IPY_MODEL_fadf9bf81380467c940325ec8d65e89f","value":"Map: 100%"}},"9611fa78c64242108da5730b0415cacf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_141461ec61af4f658ebb8f83b1432442","max":90,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9814cc9eba08428c9358a6a760fe0143","value":90}},"8c6a28bfda0d452585d01edbbe26a2f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17dfa81146074b58b86986d1d499aa5f","placeholder":"​","style":"IPY_MODEL_f350099fbfa64f44b94f924e0bc3965a","value":" 90/90 [00:00&lt;00:00, 3926.64 examples/s]"}},"b50dd8fe90194116bc9a75b83466d1c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ff6ddb260749b6934db8a4e81bbb4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fadf9bf81380467c940325ec8d65e89f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"141461ec61af4f658ebb8f83b1432442":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9814cc9eba08428c9358a6a760fe0143":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"17dfa81146074b58b86986d1d499aa5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f350099fbfa64f44b94f924e0bc3965a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}